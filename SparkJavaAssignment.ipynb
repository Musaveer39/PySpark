{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfgpfS0QmCqRNthM9Z2SPV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Musaveer39/PySpark/blob/main/SparkJavaAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "B78PpDa5px-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED6v9CGrppBr",
        "outputId": "ffa10000-ae67-4002-fbf8-29912023e70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9y0kPOMqBS1",
        "outputId": "d128ccee-4e1a-45e4-c5f9-7fbf3d10ee6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RyLyYS5qR02",
        "outputId": "aad0583c-7b73-48d1-bbc9-df02432543df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ30uQpgr85f",
        "outputId": "b6b02183-fff4-4914-f160-8f8f7c4e09ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "javac 11.0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Data Ingestion & Setup"
      ],
      "metadata": {
        "id": "-I9oC-CnsKGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SparkApp.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SparkApp {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Java Spark App\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> df = spark.read().option(\"header\", true).csv(\"input.csv\");\n",
        "        df.show();\n",
        "\n",
        "        df.write().mode(\"overwrite\").parquet(\"output_parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ma8MXGKtV_T",
        "outputId": "0a961d88-bbbd-46cb-8fdc-d3aa2f938eae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing SparkApp.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SparkApp.java\n"
      ],
      "metadata": {
        "id": "CcyRanwsvFex"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" SparkApp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAntvXaUv6kN",
        "outputId": "194ea511-e699-4741-f131-34066b336480"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:00:30 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:00:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:00:31 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:00:31 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:00:31 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:00:31 INFO SparkContext: Submitted application: Java Spark App\n",
            "25/08/06 05:00:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:00:31 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:00:31 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:00:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:00:32 INFO Utils: Successfully started service 'sparkDriver' on port 37949.\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:00:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:00:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:00:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f4f0443b-da35-4668-88a4-18b25ff33a85\n",
            "25/08/06 05:00:32 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:00:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:00:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:00:33 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 05:00:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:00:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37935.\n",
            "25/08/06 05:00:33 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:37935\n",
            "25/08/06 05:00:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:00:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:33 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:37935 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:00:34 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/input.csv.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 05:00:37 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 05:00:37 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:00:37 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4040\n",
            "25/08/06 05:00:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:00:37 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:00:37 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:00:37 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:00:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:00:37 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:00:37 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:00:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9efbc52-f621-4d41-ba8b-3601c119d1ec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Schema.java\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class Schema {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Data Ingestion Assignment\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Define schemas manually\n",
        "        StructType productLinesSchema = new StructType()\n",
        "                .add(\"productLine\", DataTypes.StringType)\n",
        "                .add(\"textDescription\", DataTypes.StringType);\n",
        "\n",
        "        StructType productsSchema = new StructType()\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"productName\", DataTypes.StringType)\n",
        "                .add(\"productLine\", DataTypes.StringType);\n",
        "\n",
        "        StructType officesSchema = new StructType()\n",
        "                .add(\"officeCode\", DataTypes.StringType)\n",
        "                .add(\"city\", DataTypes.StringType)\n",
        "                .add(\"country\", DataTypes.StringType);\n",
        "\n",
        "        StructType employeesSchema = new StructType()\n",
        "                .add(\"employeeNumber\", DataTypes.IntegerType)\n",
        "                .add(\"lastName\", DataTypes.StringType)\n",
        "                .add(\"officeCode\", DataTypes.StringType);\n",
        "\n",
        "        StructType customersSchema = new StructType()\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType)\n",
        "                .add(\"customerName\", DataTypes.StringType);\n",
        "\n",
        "        StructType paymentsSchema = new StructType()\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType)\n",
        "                .add(\"checkNumber\", DataTypes.StringType)\n",
        "                .add(\"paymentDate\", DataTypes.StringType); // consider DateType if date formatted\n",
        "\n",
        "        StructType ordersSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"orderDate\", DataTypes.StringType)\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        StructType orderDetailsSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"quantityOrdered\", DataTypes.IntegerType);\n",
        "\n",
        "        // Base paths\n",
        "        String inputPath = \"\";\n",
        "        String outputPath = \"data/parquet/\";\n",
        "\n",
        "        // Read and write all tables\n",
        "        readAndSave(spark, inputPath + \"productlines.csv\", outputPath + \"productlines\", productLinesSchema);\n",
        "        readAndSave(spark, inputPath + \"products.csv\", outputPath + \"products\", productsSchema);\n",
        "        readAndSave(spark, inputPath + \"offices.csv\", outputPath + \"offices\", officesSchema);\n",
        "        readAndSave(spark, inputPath + \"employees.csv\", outputPath + \"employees\", employeesSchema);\n",
        "        readAndSave(spark, inputPath + \"customers.csv\", outputPath + \"customers\", customersSchema);\n",
        "        readAndSave(spark, inputPath + \"payments.csv\", outputPath + \"payments\", paymentsSchema);\n",
        "        readAndSave(spark, inputPath + \"orders.csv\", outputPath + \"orders\", ordersSchema);\n",
        "        readAndSave(spark, inputPath + \"orderdetails.csv\", outputPath + \"orderdetails\", orderDetailsSchema);\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "\n",
        "    private static void readAndSave(SparkSession spark, String inputCsvPath, String outputParquetPath, StructType schema) {\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(inputCsvPath);\n",
        "\n",
        "        df.write()\n",
        "                .mode(SaveMode.Overwrite)\n",
        "                .parquet(outputParquetPath);\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c23EsRvsVHM",
        "outputId": "57ae170a-e4ca-4b10-e845-060329047291"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Schema.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Schema.java"
      ],
      "metadata": {
        "id": "q5HU6nj3szg4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" Schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cinR1ealtOq7",
        "outputId": "62bf479a-67f5-4d51-c956-4efcbbe68f94"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:09:48 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:09:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:09:49 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:09:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:09:49 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:09:49 INFO SparkContext: Submitted application: Data Ingestion Assignment\n",
            "25/08/06 05:09:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:09:49 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:09:49 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:09:49 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:09:49 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:09:49 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:09:49 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:09:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:09:49 INFO Utils: Successfully started service 'sparkDriver' on port 45353.\n",
            "25/08/06 05:09:49 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:09:50 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:09:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:09:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:09:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:09:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-31c1632b-6fb0-49ea-ba63-d5d3277d6b74\n",
            "25/08/06 05:09:50 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:09:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:09:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:09:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:09:50 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 05:09:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:09:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45153.\n",
            "25/08/06 05:09:50 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:45153\n",
            "25/08/06 05:09:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:09:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 45153, None)\n",
            "25/08/06 05:09:50 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:45153 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 45153, None)\n",
            "25/08/06 05:09:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 45153, None)\n",
            "25/08/06 05:09:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 45153, None)\n",
            "25/08/06 05:09:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:09:51 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:09:54 INFO InMemoryFileIndex: It took 147 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:09:58 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:09:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:09:58 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:09:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:09:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:09:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:09:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:09:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:09:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:09:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:09:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:09:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:09:59 INFO SparkContext: Created broadcast 0 from parquet at Schema.java:75\n",
            "25/08/06 05:09:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:09:59 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:09:59 INFO DAGScheduler: Got job 0 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:09:59 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Schema.java:75)\n",
            "25/08/06 05:09:59 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:09:59 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:09:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.4 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:10:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 76.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:10:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:10:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 05:10:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 05:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:00 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:00 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:00 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 05:10:01 INFO FileScanRDD: Reading File path: file:///content/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 05:10:01 INFO CodeGenerator: Code generated in 384.412096 ms\n",
            "25/08/06 05:10:01 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/productlines.csv\n",
            "25/08/06 05:10:02 INFO FileOutputCommitter: Saved output of task 'attempt_20250806050959655895913483099619_0000_m_000000_0' to file:/content/data/parquet/productlines/_temporary/0/task_20250806050959655895913483099619_0000_m_000000\n",
            "25/08/06 05:10:02 INFO SparkHadoopMapRedUtil: attempt_20250806050959655895913483099619_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:10:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2545 bytes result sent to driver\n",
            "25/08/06 05:10:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2457 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:02 INFO DAGScheduler: ResultStage 0 (parquet at Schema.java:75) finished in 3.032 s\n",
            "25/08/06 05:10:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 05:10:02 INFO DAGScheduler: Job 0 finished: parquet at Schema.java:75, took 3.129159 s\n",
            "25/08/06 05:10:02 INFO FileFormatWriter: Start to commit write Job 3d830c9e-2871-4647-8d5f-57f045c3db1b.\n",
            "25/08/06 05:10:02 INFO FileFormatWriter: Write Job 3d830c9e-2871-4647-8d5f-57f045c3db1b committed. Elapsed time: 31 ms.\n",
            "25/08/06 05:10:02 INFO FileFormatWriter: Finished processing stats for write job 3d830c9e-2871-4647-8d5f-57f045c3db1b.\n",
            "25/08/06 05:10:02 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:02 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:02 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:10:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:10:03 INFO SparkContext: Created broadcast 2 from parquet at Schema.java:75\n",
            "25/08/06 05:10:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:03 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Got job 1 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 210.6 KiB, free 1766.7 MiB)\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.2 KiB, free 1766.6 MiB)\n",
            "25/08/06 05:10:03 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:10:03 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7908 bytes) \n",
            "25/08/06 05:10:03 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:03 INFO FileScanRDD: Reading File path: file:///content/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 05:10:03 INFO CodeGenerator: Code generated in 19.975066 ms\n",
            "25/08/06 05:10:03 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 9, schema size: 3\n",
            "CSV file: file:///content/products.csv\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: Saved output of task 'attempt_202508060510036562178569514837195_0001_m_000000_1' to file:/content/data/parquet/products/_temporary/0/task_202508060510036562178569514837195_0001_m_000000\n",
            "25/08/06 05:10:03 INFO SparkHadoopMapRedUtil: attempt_202508060510036562178569514837195_0001_m_000000_1: Committed. Elapsed time: 3 ms.\n",
            "25/08/06 05:10:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2502 bytes result sent to driver\n",
            "25/08/06 05:10:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 269 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:03 INFO DAGScheduler: ResultStage 1 (parquet at Schema.java:75) finished in 0.345 s\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Job 1 finished: parquet at Schema.java:75, took 0.353962 s\n",
            "25/08/06 05:10:03 INFO FileFormatWriter: Start to commit write Job 5436c3e1-72b2-4f84-8a20-167f7175e5ed.\n",
            "25/08/06 05:10:03 INFO FileFormatWriter: Write Job 5436c3e1-72b2-4f84-8a20-167f7175e5ed committed. Elapsed time: 16 ms.\n",
            "25/08/06 05:10:03 INFO FileFormatWriter: Finished processing stats for write job 5436c3e1-72b2-4f84-8a20-167f7175e5ed.\n",
            "25/08/06 05:10:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:03 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:10:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:10:03 INFO SparkContext: Created broadcast 4 from parquet at Schema.java:75\n",
            "25/08/06 05:10:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:03 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Got job 2 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 210.6 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.2 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:10:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.2 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:10:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7907 bytes) \n",
            "25/08/06 05:10:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:03 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:03 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary country (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:03 INFO FileScanRDD: Reading File path: file:///content/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 05:10:03 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 9, schema size: 3\n",
            "CSV file: file:///content/offices.csv\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: Saved output of task 'attempt_202508060510037014726464898196980_0002_m_000000_2' to file:/content/data/parquet/offices/_temporary/0/task_202508060510037014726464898196980_0002_m_000000\n",
            "25/08/06 05:10:03 INFO SparkHadoopMapRedUtil: attempt_202508060510037014726464898196980_0002_m_000000_2: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:10:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2502 bytes result sent to driver\n",
            "25/08/06 05:10:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 142 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:03 INFO DAGScheduler: ResultStage 2 (parquet at Schema.java:75) finished in 0.211 s\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 05:10:03 INFO DAGScheduler: Job 2 finished: parquet at Schema.java:75, took 0.219096 s\n",
            "25/08/06 05:10:03 INFO FileFormatWriter: Start to commit write Job 8a6b9059-2e7a-449f-b4f7-afe7ab38b4f7.\n",
            "25/08/06 05:10:03 INFO FileFormatWriter: Write Job 8a6b9059-2e7a-449f-b4f7-afe7ab38b4f7 committed. Elapsed time: 19 ms.\n",
            "25/08/06 05:10:03 INFO FileFormatWriter: Finished processing stats for write job 8a6b9059-2e7a-449f-b4f7-afe7ab38b4f7.\n",
            "25/08/06 05:10:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:03 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 198.5 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:10:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:10:04 INFO SparkContext: Created broadcast 6 from parquet at Schema.java:75\n",
            "25/08/06 05:10:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:04 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Got job 3 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 210.7 KiB, free 1765.6 MiB)\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1765.6 MiB)\n",
            "25/08/06 05:10:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.3 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:10:04 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
            "25/08/06 05:10:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:04 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:04 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 employeeNumber;\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:04 INFO FileScanRDD: Reading File path: file:///content/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 05:10:04 INFO CodeGenerator: Code generated in 23.907239 ms\n",
            "25/08/06 05:10:04 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 8, schema size: 3\n",
            "CSV file: file:///content/employees.csv\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: Saved output of task 'attempt_202508060510046529926117162916545_0003_m_000000_3' to file:/content/data/parquet/employees/_temporary/0/task_202508060510046529926117162916545_0003_m_000000\n",
            "25/08/06 05:10:04 INFO SparkHadoopMapRedUtil: attempt_202508060510046529926117162916545_0003_m_000000_3: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:10:04 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2502 bytes result sent to driver\n",
            "25/08/06 05:10:04 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 163 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:04 INFO DAGScheduler: ResultStage 3 (parquet at Schema.java:75) finished in 0.227 s\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Job 3 finished: parquet at Schema.java:75, took 0.243977 s\n",
            "25/08/06 05:10:04 INFO FileFormatWriter: Start to commit write Job ad433de3-4c8c-4748-a952-7db4aff54d5d.\n",
            "25/08/06 05:10:04 INFO FileFormatWriter: Write Job ad433de3-4c8c-4748-a952-7db4aff54d5d committed. Elapsed time: 16 ms.\n",
            "25/08/06 05:10:04 INFO FileFormatWriter: Finished processing stats for write job ad433de3-4c8c-4748-a952-7db4aff54d5d.\n",
            "25/08/06 05:10:04 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:04 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 198.5 KiB, free 1765.4 MiB)\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 05:10:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:10:04 INFO SparkContext: Created broadcast 8 from parquet at Schema.java:75\n",
            "25/08/06 05:10:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:04 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Got job 4 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 210.4 KiB, free 1765.1 MiB)\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 76.0 KiB, free 1765.1 MiB)\n",
            "25/08/06 05:10:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:10:04 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
            "25/08/06 05:10:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:04 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:04 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary customerName (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:04 INFO FileScanRDD: Reading File path: file:///content/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 05:10:04 INFO CodeGenerator: Code generated in 33.410565 ms\n",
            "25/08/06 05:10:04 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 13, schema size: 2\n",
            "CSV file: file:///content/customers.csv\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: Saved output of task 'attempt_20250806051004133581214999476770_0004_m_000000_4' to file:/content/data/parquet/customers/_temporary/0/task_20250806051004133581214999476770_0004_m_000000\n",
            "25/08/06 05:10:04 INFO SparkHadoopMapRedUtil: attempt_20250806051004133581214999476770_0004_m_000000_4: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:10:04 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2502 bytes result sent to driver\n",
            "25/08/06 05:10:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 181 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:04 INFO DAGScheduler: ResultStage 4 (parquet at Schema.java:75) finished in 0.255 s\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Job 4 finished: parquet at Schema.java:75, took 0.260782 s\n",
            "25/08/06 05:10:04 INFO FileFormatWriter: Start to commit write Job f2696ecb-72dc-4646-9786-7f6310f8120c.\n",
            "25/08/06 05:10:04 INFO FileFormatWriter: Write Job f2696ecb-72dc-4646-9786-7f6310f8120c committed. Elapsed time: 10 ms.\n",
            "25/08/06 05:10:04 INFO FileFormatWriter: Finished processing stats for write job f2696ecb-72dc-4646-9786-7f6310f8120c.\n",
            "25/08/06 05:10:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:04 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.5 KiB, free 1764.9 MiB)\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.8 MiB)\n",
            "25/08/06 05:10:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:10:04 INFO SparkContext: Created broadcast 10 from parquet at Schema.java:75\n",
            "25/08/06 05:10:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:04 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Got job 5 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 210.7 KiB, free 1764.6 MiB)\n",
            "25/08/06 05:10:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1764.6 MiB)\n",
            "25/08/06 05:10:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.3 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:10:04 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:04 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:04 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7908 bytes) \n",
            "25/08/06 05:10:04 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:04 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:04 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:04 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:04 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional binary paymentDate (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:05 INFO FileScanRDD: Reading File path: file:///content/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 05:10:05 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 3\n",
            "CSV file: file:///content/payments.csv\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 0c0ac0b11caf:45153 in memory (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: Saved output of task 'attempt_202508060510042046035718503452760_0005_m_000000_5' to file:/content/data/parquet/payments/_temporary/0/task_202508060510042046035718503452760_0005_m_000000\n",
            "25/08/06 05:10:05 INFO SparkHadoopMapRedUtil: attempt_202508060510042046035718503452760_0005_m_000000_5: Committed. Elapsed time: 0 ms.\n",
            "25/08/06 05:10:05 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2502 bytes result sent to driver\n",
            "25/08/06 05:10:05 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 357 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:05 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:05 INFO DAGScheduler: ResultStage 5 (parquet at Schema.java:75) finished in 0.423 s\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Job 5 finished: parquet at Schema.java:75, took 0.429500 s\n",
            "25/08/06 05:10:05 INFO FileFormatWriter: Start to commit write Job 384c0e2a-e7f7-483f-9b1c-9910161d0f93.\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 0c0ac0b11caf:45153 in memory (size: 76.2 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:10:05 INFO FileFormatWriter: Write Job 384c0e2a-e7f7-483f-9b1c-9910161d0f93 committed. Elapsed time: 15 ms.\n",
            "25/08/06 05:10:05 INFO FileFormatWriter: Finished processing stats for write job 384c0e2a-e7f7-483f-9b1c-9910161d0f93.\n",
            "25/08/06 05:10:05 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 0c0ac0b11caf:45153 in memory (size: 76.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 0c0ac0b11caf:45153 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:10:05 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 0c0ac0b11caf:45153 in memory (size: 76.2 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:10:05 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0c0ac0b11caf:45153 in memory (size: 76.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:10:05 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.5 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:10:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:10:05 INFO SparkContext: Created broadcast 12 from parquet at Schema.java:75\n",
            "25/08/06 05:10:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 0c0ac0b11caf:45153 in memory (size: 34.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:10:05 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Got job 6 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 0c0ac0b11caf:45153 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:10:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 210.7 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:10:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.3 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:10:05 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:05 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:05 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7906 bytes) \n",
            "25/08/06 05:10:05 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 0c0ac0b11caf:45153 in memory (size: 76.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:10:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:05 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:05 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:05 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:05 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary orderDate (STRING);\n",
            "  optional int32 customerNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0c0ac0b11caf:45153 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:10:05 INFO FileScanRDD: Reading File path: file:///content/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 05:10:05 INFO CodeGenerator: Code generated in 34.22621 ms\n",
            "25/08/06 05:10:05 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 7, schema size: 3\n",
            "CSV file: file:///content/orders.csv\n",
            "25/08/06 05:10:05 INFO FileOutputCommitter: Saved output of task 'attempt_20250806051005253664298365323145_0006_m_000000_6' to file:/content/data/parquet/orders/_temporary/0/task_20250806051005253664298365323145_0006_m_000000\n",
            "25/08/06 05:10:05 INFO SparkHadoopMapRedUtil: attempt_20250806051005253664298365323145_0006_m_000000_6: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:10:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2459 bytes result sent to driver\n",
            "25/08/06 05:10:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 236 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:05 INFO DAGScheduler: ResultStage 6 (parquet at Schema.java:75) finished in 0.304 s\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 05:10:05 INFO DAGScheduler: Job 6 finished: parquet at Schema.java:75, took 0.317158 s\n",
            "25/08/06 05:10:05 INFO FileFormatWriter: Start to commit write Job 43f153f8-fd4a-40f6-9be2-b487938a3fa8.\n",
            "25/08/06 05:10:05 INFO FileFormatWriter: Write Job 43f153f8-fd4a-40f6-9be2-b487938a3fa8 committed. Elapsed time: 21 ms.\n",
            "25/08/06 05:10:05 INFO FileFormatWriter: Finished processing stats for write job 43f153f8-fd4a-40f6-9be2-b487938a3fa8.\n",
            "25/08/06 05:10:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:10:05 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:10:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:10:06 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:06 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:10:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 05:10:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 0c0ac0b11caf:45153 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:10:06 INFO SparkContext: Created broadcast 14 from parquet at Schema.java:75\n",
            "25/08/06 05:10:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:10:06 INFO SparkContext: Starting job: parquet at Schema.java:75\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Got job 7 (parquet at Schema.java:75) with 1 output partitions\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at Schema.java:75)\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at parquet at Schema.java:75), which has no missing parents\n",
            "25/08/06 05:10:06 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 210.7 KiB, free 1766.2 MiB)\n",
            "25/08/06 05:10:06 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:10:06 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 0c0ac0b11caf:45153 (size: 76.3 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:10:06 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at parquet at Schema.java:75) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:10:06 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:10:06 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 05:10:06 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:10:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:10:06 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:06 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:10:06 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:10:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary productCode (STRING);\n",
            "  optional int32 quantityOrdered;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:10:06 INFO FileScanRDD: Reading File path: file:///content/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 05:10:06 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 5, schema size: 3\n",
            "CSV file: file:///content/orderdetails.csv\n",
            "25/08/06 05:10:06 INFO FileOutputCommitter: Saved output of task 'attempt_202508060510064838707899324273405_0007_m_000000_7' to file:/content/data/parquet/orderdetails/_temporary/0/task_202508060510064838707899324273405_0007_m_000000\n",
            "25/08/06 05:10:06 INFO SparkHadoopMapRedUtil: attempt_202508060510064838707899324273405_0007_m_000000_7: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 05:10:06 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2459 bytes result sent to driver\n",
            "25/08/06 05:10:06 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 440 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:10:06 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:10:06 INFO DAGScheduler: ResultStage 7 (parquet at Schema.java:75) finished in 0.510 s\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:10:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 05:10:06 INFO DAGScheduler: Job 7 finished: parquet at Schema.java:75, took 0.522141 s\n",
            "25/08/06 05:10:06 INFO FileFormatWriter: Start to commit write Job 7cc52203-a2e4-4183-839c-dc83531c2aa0.\n",
            "25/08/06 05:10:06 INFO FileFormatWriter: Write Job 7cc52203-a2e4-4183-839c-dc83531c2aa0 committed. Elapsed time: 31 ms.\n",
            "25/08/06 05:10:06 INFO FileFormatWriter: Finished processing stats for write job 7cc52203-a2e4-4183-839c-dc83531c2aa0.\n",
            "25/08/06 05:10:06 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:10:06 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4040\n",
            "25/08/06 05:10:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:10:06 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:10:06 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:10:06 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:10:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:10:06 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:10:06 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:10:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff9315ae-e619-4bf6-a1be-9b2dcf8a928f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXwc6bXEx5Xr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}