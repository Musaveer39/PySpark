{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B78PpDa5px-2"
      ],
      "authorship_tag": "ABX9TyOaYPDjsk1Sry/0ll3ZgtiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Musaveer39/PySpark/blob/main/SparkJavaAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "B78PpDa5px-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED6v9CGrppBr",
        "outputId": "ffa10000-ae67-4002-fbf8-29912023e70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9y0kPOMqBS1",
        "outputId": "d128ccee-4e1a-45e4-c5f9-7fbf3d10ee6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RyLyYS5qR02",
        "outputId": "aad0583c-7b73-48d1-bbc9-df02432543df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ30uQpgr85f",
        "outputId": "b6b02183-fff4-4914-f160-8f8f7c4e09ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "javac 11.0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Data Ingestion & Setup"
      ],
      "metadata": {
        "id": "-I9oC-CnsKGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SparkApp.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SparkApp {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Java Spark App\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> df = spark.read().option(\"header\", true).csv(\"input.csv\");\n",
        "        df.show();\n",
        "\n",
        "        df.write().mode(\"overwrite\").parquet(\"output_parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ma8MXGKtV_T",
        "outputId": "0a961d88-bbbd-46cb-8fdc-d3aa2f938eae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing SparkApp.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SparkApp.java\n"
      ],
      "metadata": {
        "id": "CcyRanwsvFex"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" SparkApp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAntvXaUv6kN",
        "outputId": "194ea511-e699-4741-f131-34066b336480"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:00:30 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:00:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:00:31 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:00:31 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:00:31 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:00:31 INFO SparkContext: Submitted application: Java Spark App\n",
            "25/08/06 05:00:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:00:31 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:00:31 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:00:31 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:00:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:00:32 INFO Utils: Successfully started service 'sparkDriver' on port 37949.\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:00:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:00:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:00:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f4f0443b-da35-4668-88a4-18b25ff33a85\n",
            "25/08/06 05:00:32 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:00:32 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:00:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:00:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:00:33 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 05:00:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:00:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37935.\n",
            "25/08/06 05:00:33 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:37935\n",
            "25/08/06 05:00:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:00:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:33 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:37935 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 37935, None)\n",
            "25/08/06 05:00:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:00:34 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/input.csv.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 05:00:37 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 05:00:37 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:00:37 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4040\n",
            "25/08/06 05:00:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:00:37 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:00:37 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:00:37 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:00:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:00:37 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:00:37 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:00:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9efbc52-f621-4d41-ba8b-3601c119d1ec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Schema.java\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class Schema {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Data Ingestion Assignment\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Define schemas manually\n",
        "        StructType productLinesSchema = new StructType()\n",
        "                .add(\"productLine\", DataTypes.StringType)\n",
        "                .add(\"textDescription\", DataTypes.StringType);\n",
        "\n",
        "        StructType productsSchema = new StructType()\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"productName\", DataTypes.StringType)\n",
        "                .add(\"productLine\", DataTypes.StringType);\n",
        "\n",
        "        StructType officesSchema = new StructType()\n",
        "                .add(\"officeCode\", DataTypes.StringType)\n",
        "                .add(\"city\", DataTypes.StringType)\n",
        "                .add(\"country\", DataTypes.StringType);\n",
        "\n",
        "        StructType employeesSchema = new StructType()\n",
        "                .add(\"employeeNumber\", DataTypes.IntegerType)\n",
        "                .add(\"lastName\", DataTypes.StringType)\n",
        "                .add(\"officeCode\", DataTypes.StringType);\n",
        "\n",
        "        StructType customersSchema = new StructType()\n",
        "    .add(\"customerNumber\", DataTypes.IntegerType, true)\n",
        "    .add(\"customerName\", DataTypes.StringType, true)\n",
        "    .add(\"contactLastName\", DataTypes.StringType, true)\n",
        "    .add(\"contactFirstName\", DataTypes.StringType, true)\n",
        "    .add(\"phone\", DataTypes.StringType, true)\n",
        "    .add(\"addressLine1\", DataTypes.StringType, true)\n",
        "    .add(\"addressLine2\", DataTypes.StringType, true)\n",
        "    .add(\"city\", DataTypes.StringType, true)\n",
        "    .add(\"state\", DataTypes.StringType, true)\n",
        "    .add(\"postalCode\", DataTypes.StringType, true)\n",
        "    .add(\"country\", DataTypes.StringType, true)\n",
        "    .add(\"salesRepEmployeeNumber\", DataTypes.IntegerType, true)\n",
        "    .add(\"creditLimit\", DataTypes.DoubleType, true);\n",
        "\n",
        "        StructType paymentsSchema = new StructType()\n",
        "              .add(\"customerNumber\", DataTypes.IntegerType)\n",
        "              .add(\"checkNumber\", DataTypes.StringType)\n",
        "              .add(\"paymentDate\", DataTypes.DateType)\n",
        "              .add(\"amount\", DataTypes.DoubleType);\n",
        "\n",
        "        StructType ordersSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"orderDate\", DataTypes.StringType)\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        StructType orderDetailsSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"quantityOrdered\", DataTypes.IntegerType)\n",
        "                .add(\"priceEach\", DataTypes.DoubleType)\n",
        "                .add(\"orderLineNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        // Base paths\n",
        "        String inputPath = \"\";\n",
        "        String outputPath = \"data/parquet/\";\n",
        "\n",
        "        // Read and write all tables\n",
        "        readAndSave(spark, inputPath + \"productlines.csv\", outputPath + \"productlines\", productLinesSchema);\n",
        "        readAndSave(spark, inputPath + \"products.csv\", outputPath + \"products\", productsSchema);\n",
        "        readAndSave(spark, inputPath + \"offices.csv\", outputPath + \"offices\", officesSchema);\n",
        "        readAndSave(spark, inputPath + \"employees.csv\", outputPath + \"employees\", employeesSchema);\n",
        "        readAndSave(spark, inputPath + \"customers.csv\", outputPath + \"customers\", customersSchema);\n",
        "        readAndSave(spark, inputPath + \"payments.csv\", outputPath + \"payments\", paymentsSchema);\n",
        "        readAndSave(spark, inputPath + \"orders.csv\", outputPath + \"orders\", ordersSchema);\n",
        "        readAndSave(spark, inputPath + \"orderdetails.csv\", outputPath + \"orderdetails\", orderDetailsSchema);\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "\n",
        "    private static void readAndSave(SparkSession spark, String inputCsvPath, String outputParquetPath, StructType schema) {\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(inputCsvPath);\n",
        "\n",
        "        df.write()\n",
        "                .mode(SaveMode.Overwrite)\n",
        "                .parquet(outputParquetPath);\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c23EsRvsVHM",
        "outputId": "91d21af7-e598-48cc-a34d-a09f81d50bdc"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Schema.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Schema.java"
      ],
      "metadata": {
        "id": "q5HU6nj3szg4"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" Schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cinR1ealtOq7",
        "outputId": "dd6f7c44-69b6-43a9-f274-869656cba1eb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 06:41:37 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 06:41:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 06:41:37 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:41:37 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 06:41:37 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:41:37 INFO SparkContext: Submitted application: Data Ingestion Assignment\n",
            "25/08/06 06:41:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 06:41:37 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 06:41:37 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 06:41:37 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 06:41:37 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 06:41:37 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 06:41:37 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 06:41:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 06:41:38 INFO Utils: Successfully started service 'sparkDriver' on port 35155.\n",
            "25/08/06 06:41:38 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 06:41:38 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 06:41:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 06:41:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 06:41:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 06:41:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4759d9f8-e23a-48bc-bcd2-473aedfdf781\n",
            "25/08/06 06:41:38 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 06:41:38 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 06:41:38 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 06:41:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 06:41:38 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 06:41:39 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 06:41:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 06:41:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45049.\n",
            "25/08/06 06:41:39 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:45049\n",
            "25/08/06 06:41:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 06:41:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 45049, None)\n",
            "25/08/06 06:41:39 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:45049 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 45049, None)\n",
            "25/08/06 06:41:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 45049, None)\n",
            "25/08/06 06:41:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 45049, None)\n",
            "25/08/06 06:41:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 06:41:39 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 06:41:42 INFO InMemoryFileIndex: It took 173 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:46 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:46 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:47 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:41:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:41:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:41:47 INFO SparkContext: Created broadcast 0 from parquet at Schema.java:89\n",
            "25/08/06 06:41:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:47 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:47 INFO DAGScheduler: Got job 0 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:47 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 210.4 KiB, free 1767.2 MiB)\n",
            "25/08/06 06:41:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 75.9 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:41:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0c0ac0b11caf:45049 (size: 75.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:41:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 06:41:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 06:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:48 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:48 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:48 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:48 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 06:41:49 INFO FileScanRDD: Reading File path: file:///content/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 06:41:49 INFO CodeGenerator: Code generated in 377.991003 ms\n",
            "25/08/06 06:41:49 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 4, schema size: 2\n",
            "CSV file: file:///content/productlines.csv\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641475111243461387459627_0000_m_000000_0' to file:/content/data/parquet/productlines/_temporary/0/task_202508060641475111243461387459627_0000_m_000000\n",
            "25/08/06 06:41:50 INFO SparkHadoopMapRedUtil: attempt_202508060641475111243461387459627_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:41:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2545 bytes result sent to driver\n",
            "25/08/06 06:41:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2107 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:50 INFO DAGScheduler: ResultStage 0 (parquet at Schema.java:89) finished in 2.482 s\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Job 0 finished: parquet at Schema.java:89, took 2.592168 s\n",
            "25/08/06 06:41:50 INFO FileFormatWriter: Start to commit write Job 886a93fe-51c6-433d-a09f-869cb3848fa4.\n",
            "25/08/06 06:41:50 INFO FileFormatWriter: Write Job 886a93fe-51c6-433d-a09f-869cb3848fa4 committed. Elapsed time: 34 ms.\n",
            "25/08/06 06:41:50 INFO FileFormatWriter: Finished processing stats for write job 886a93fe-51c6-433d-a09f-869cb3848fa4.\n",
            "25/08/06 06:41:50 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:50 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 06:41:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 06:41:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:41:50 INFO SparkContext: Created broadcast 2 from parquet at Schema.java:89\n",
            "25/08/06 06:41:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:50 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Got job 1 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 210.6 KiB, free 1766.7 MiB)\n",
            "25/08/06 06:41:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.1 KiB, free 1766.6 MiB)\n",
            "25/08/06 06:41:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0c0ac0b11caf:45049 (size: 76.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:41:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7908 bytes) \n",
            "25/08/06 06:41:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:50 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:50 INFO FileScanRDD: Reading File path: file:///content/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 06:41:50 INFO CodeGenerator: Code generated in 47.927296 ms\n",
            "25/08/06 06:41:50 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 9, schema size: 3\n",
            "CSV file: file:///content/products.csv\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641508596571499393035755_0001_m_000000_1' to file:/content/data/parquet/products/_temporary/0/task_202508060641508596571499393035755_0001_m_000000\n",
            "25/08/06 06:41:51 INFO SparkHadoopMapRedUtil: attempt_202508060641508596571499393035755_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:41:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2502 bytes result sent to driver\n",
            "25/08/06 06:41:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 268 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:51 INFO DAGScheduler: ResultStage 1 (parquet at Schema.java:89) finished in 0.344 s\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Job 1 finished: parquet at Schema.java:89, took 0.351112 s\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Start to commit write Job 6a981e7b-d247-4865-81cc-ea543933f1d5.\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Write Job 6a981e7b-d247-4865-81cc-ea543933f1d5 committed. Elapsed time: 13 ms.\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Finished processing stats for write job 6a981e7b-d247-4865-81cc-ea543933f1d5.\n",
            "25/08/06 06:41:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 06:41:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:41:51 INFO SparkContext: Created broadcast 4 from parquet at Schema.java:89\n",
            "25/08/06 06:41:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:51 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Got job 2 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 210.6 KiB, free 1766.2 MiB)\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.2 KiB, free 1766.1 MiB)\n",
            "25/08/06 06:41:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0c0ac0b11caf:45049 (size: 76.2 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:41:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7907 bytes) \n",
            "25/08/06 06:41:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:51 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary country (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:51 INFO FileScanRDD: Reading File path: file:///content/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 06:41:51 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 9, schema size: 3\n",
            "CSV file: file:///content/offices.csv\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641516428532792366478906_0002_m_000000_2' to file:/content/data/parquet/offices/_temporary/0/task_202508060641516428532792366478906_0002_m_000000\n",
            "25/08/06 06:41:51 INFO SparkHadoopMapRedUtil: attempt_202508060641516428532792366478906_0002_m_000000_2: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:41:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2502 bytes result sent to driver\n",
            "25/08/06 06:41:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 91 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:51 INFO DAGScheduler: ResultStage 2 (parquet at Schema.java:89) finished in 0.145 s\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Job 2 finished: parquet at Schema.java:89, took 0.151900 s\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Start to commit write Job 3a717a15-bf7e-4013-92bc-dc5f0185cd34.\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Write Job 3a717a15-bf7e-4013-92bc-dc5f0185cd34 committed. Elapsed time: 15 ms.\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Finished processing stats for write job 3a717a15-bf7e-4013-92bc-dc5f0185cd34.\n",
            "25/08/06 06:41:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 198.5 KiB, free 1765.9 MiB)\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.9 MiB)\n",
            "25/08/06 06:41:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:41:51 INFO SparkContext: Created broadcast 6 from parquet at Schema.java:89\n",
            "25/08/06 06:41:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:51 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Got job 3 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 210.7 KiB, free 1765.6 MiB)\n",
            "25/08/06 06:41:51 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1765.6 MiB)\n",
            "25/08/06 06:41:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0c0ac0b11caf:45049 (size: 76.3 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:41:51 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:51 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
            "25/08/06 06:41:51 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:51 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 employeeNumber;\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:51 INFO FileScanRDD: Reading File path: file:///content/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 06:41:51 INFO CodeGenerator: Code generated in 17.889495 ms\n",
            "25/08/06 06:41:51 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 8, schema size: 3\n",
            "CSV file: file:///content/employees.csv\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641515057557502311729069_0003_m_000000_3' to file:/content/data/parquet/employees/_temporary/0/task_202508060641515057557502311729069_0003_m_000000\n",
            "25/08/06 06:41:51 INFO SparkHadoopMapRedUtil: attempt_202508060641515057557502311729069_0003_m_000000_3: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:41:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2502 bytes result sent to driver\n",
            "25/08/06 06:41:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 151 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:51 INFO DAGScheduler: ResultStage 3 (parquet at Schema.java:89) finished in 0.246 s\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 06:41:51 INFO DAGScheduler: Job 3 finished: parquet at Schema.java:89, took 0.252918 s\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Start to commit write Job 2095fcd6-bce8-4aef-8a60-8030b3c0d791.\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Write Job 2095fcd6-bce8-4aef-8a60-8030b3c0d791 committed. Elapsed time: 10 ms.\n",
            "25/08/06 06:41:51 INFO FileFormatWriter: Finished processing stats for write job 2095fcd6-bce8-4aef-8a60-8030b3c0d791.\n",
            "25/08/06 06:41:51 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 198.5 KiB, free 1765.4 MiB)\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1765.3 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:41:52 INFO SparkContext: Created broadcast 8 from parquet at Schema.java:89\n",
            "25/08/06 06:41:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:52 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Got job 4 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 213.1 KiB, free 1765.1 MiB)\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 77.0 KiB, free 1765.1 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 0c0ac0b11caf:45049 (size: 77.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:41:52 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:52 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:52 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
            "25/08/06 06:41:52 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:52 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional int32 salesRepEmployeeNumber;\n",
            "  optional double creditLimit;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:52 INFO FileScanRDD: Reading File path: file:///content/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 06:41:52 INFO CodeGenerator: Code generated in 36.570527 ms\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641525851406954577186999_0004_m_000000_4' to file:/content/data/parquet/customers/_temporary/0/task_202508060641525851406954577186999_0004_m_000000\n",
            "25/08/06 06:41:52 INFO SparkHadoopMapRedUtil: attempt_202508060641525851406954577186999_0004_m_000000_4: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:41:52 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2502 bytes result sent to driver\n",
            "25/08/06 06:41:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 272 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:52 INFO DAGScheduler: ResultStage 4 (parquet at Schema.java:89) finished in 0.343 s\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Job 4 finished: parquet at Schema.java:89, took 0.349561 s\n",
            "25/08/06 06:41:52 INFO FileFormatWriter: Start to commit write Job 15c6439d-a62b-4b07-ab64-6f97134f5e6d.\n",
            "25/08/06 06:41:52 INFO FileFormatWriter: Write Job 15c6439d-a62b-4b07-ab64-6f97134f5e6d committed. Elapsed time: 16 ms.\n",
            "25/08/06 06:41:52 INFO FileFormatWriter: Finished processing stats for write job 15c6439d-a62b-4b07-ab64-6f97134f5e6d.\n",
            "25/08/06 06:41:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:52 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:52 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.5 KiB, free 1764.9 MiB)\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1764.8 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.0 MiB)\n",
            "25/08/06 06:41:52 INFO SparkContext: Created broadcast 10 from parquet at Schema.java:89\n",
            "25/08/06 06:41:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:52 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Got job 5 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 211.0 KiB, free 1764.6 MiB)\n",
            "25/08/06 06:41:52 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1764.6 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 0c0ac0b11caf:45049 (size: 76.4 KiB, free: 1767.0 MiB)\n",
            "25/08/06 06:41:52 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:52 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:52 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7908 bytes) \n",
            "25/08/06 06:41:52 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:52 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"date\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"amount\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional int32 paymentDate (DATE);\n",
            "  optional double amount;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:52 INFO FileScanRDD: Reading File path: file:///content/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 0c0ac0b11caf:45049 in memory (size: 76.2 KiB, free: 1767.0 MiB)\n",
            "25/08/06 06:41:52 INFO CodeGenerator: Code generated in 45.705413 ms\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 0c0ac0b11caf:45049 in memory (size: 76.3 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0c0ac0b11caf:45049 in memory (size: 34.0 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 0c0ac0b11caf:45049 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:41:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 0c0ac0b11caf:45049 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 0c0ac0b11caf:45049 in memory (size: 34.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 0c0ac0b11caf:45049 in memory (size: 76.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 0c0ac0b11caf:45049 in memory (size: 77.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 0c0ac0b11caf:45049 in memory (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641526535022830110275720_0005_m_000000_5' to file:/content/data/parquet/payments/_temporary/0/task_202508060641526535022830110275720_0005_m_000000\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0c0ac0b11caf:45049 in memory (size: 75.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:41:53 INFO SparkHadoopMapRedUtil: attempt_202508060641526535022830110275720_0005_m_000000_5: Committed. Elapsed time: 3 ms.\n",
            "25/08/06 06:41:53 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2502 bytes result sent to driver\n",
            "25/08/06 06:41:53 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 548 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:53 INFO DAGScheduler: ResultStage 5 (parquet at Schema.java:89) finished in 0.599 s\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Job 5 finished: parquet at Schema.java:89, took 0.604974 s\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Start to commit write Job 77e25628-17ae-43ee-a7de-c69622d30e89.\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Write Job 77e25628-17ae-43ee-a7de-c69622d30e89 committed. Elapsed time: 15 ms.\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Finished processing stats for write job 77e25628-17ae-43ee-a7de-c69622d30e89.\n",
            "25/08/06 06:41:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.5 KiB, free 1766.9 MiB)\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.9 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:41:53 INFO SparkContext: Created broadcast 12 from parquet at Schema.java:89\n",
            "25/08/06 06:41:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:53 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Got job 6 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 210.7 KiB, free 1766.7 MiB)\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 1766.6 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 0c0ac0b11caf:45049 (size: 76.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:41:53 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7906 bytes) \n",
            "25/08/06 06:41:53 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:53 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary orderDate (STRING);\n",
            "  optional int32 customerNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:53 INFO FileScanRDD: Reading File path: file:///content/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 06:41:53 INFO CodeGenerator: Code generated in 17.723842 ms\n",
            "25/08/06 06:41:53 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
            " Header length: 7, schema size: 3\n",
            "CSV file: file:///content/orders.csv\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641532672025968557795862_0006_m_000000_6' to file:/content/data/parquet/orders/_temporary/0/task_202508060641532672025968557795862_0006_m_000000\n",
            "25/08/06 06:41:53 INFO SparkHadoopMapRedUtil: attempt_202508060641532672025968557795862_0006_m_000000_6: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:41:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2459 bytes result sent to driver\n",
            "25/08/06 06:41:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 212 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:53 INFO DAGScheduler: ResultStage 6 (parquet at Schema.java:89) finished in 0.254 s\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Job 6 finished: parquet at Schema.java:89, took 0.260329 s\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Start to commit write Job c7959fdb-dca1-4fc0-973f-6a94b16985fa.\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Write Job c7959fdb-dca1-4fc0-973f-6a94b16985fa committed. Elapsed time: 13 ms.\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Finished processing stats for write job c7959fdb-dca1-4fc0-973f-6a94b16985fa.\n",
            "25/08/06 06:41:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:41:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:41:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:41:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.5 KiB, free 1766.4 MiB)\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 1766.4 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 0c0ac0b11caf:45049 (size: 34.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:41:53 INFO SparkContext: Created broadcast 14 from parquet at Schema.java:89\n",
            "25/08/06 06:41:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:41:53 INFO SparkContext: Starting job: parquet at Schema.java:89\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Got job 7 (parquet at Schema.java:89) with 1 output partitions\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at Schema.java:89)\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at parquet at Schema.java:89), which has no missing parents\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 211.2 KiB, free 1766.2 MiB)\n",
            "25/08/06 06:41:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 1766.1 MiB)\n",
            "25/08/06 06:41:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 0c0ac0b11caf:45049 (size: 76.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:41:53 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at parquet at Schema.java:89) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:41:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7912 bytes) \n",
            "25/08/06 06:41:53 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:41:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:41:53 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:53 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:41:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:41:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 orderNumber;\n",
            "  optional binary productCode (STRING);\n",
            "  optional int32 quantityOrdered;\n",
            "  optional double priceEach;\n",
            "  optional int32 orderLineNumber;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:41:53 INFO FileScanRDD: Reading File path: file:///content/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 06:41:53 INFO CodeGenerator: Code generated in 17.13785 ms\n",
            "25/08/06 06:41:53 INFO FileOutputCommitter: Saved output of task 'attempt_202508060641534950295007124486842_0007_m_000000_7' to file:/content/data/parquet/orderdetails/_temporary/0/task_202508060641534950295007124486842_0007_m_000000\n",
            "25/08/06 06:41:53 INFO SparkHadoopMapRedUtil: attempt_202508060641534950295007124486842_0007_m_000000_7: Committed. Elapsed time: 3 ms.\n",
            "25/08/06 06:41:53 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2459 bytes result sent to driver\n",
            "25/08/06 06:41:53 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 196 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:41:53 INFO DAGScheduler: ResultStage 7 (parquet at Schema.java:89) finished in 0.250 s\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:41:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 06:41:53 INFO DAGScheduler: Job 7 finished: parquet at Schema.java:89, took 0.256147 s\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Start to commit write Job 472a3716-c083-4269-a127-d9ae02e1013c.\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Write Job 472a3716-c083-4269-a127-d9ae02e1013c committed. Elapsed time: 26 ms.\n",
            "25/08/06 06:41:53 INFO FileFormatWriter: Finished processing stats for write job 472a3716-c083-4269-a127-d9ae02e1013c.\n",
            "25/08/06 06:41:54 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 06:41:54 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4041\n",
            "25/08/06 06:41:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 06:41:54 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 06:41:54 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 06:41:54 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 06:41:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 06:41:54 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 06:41:54 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 06:41:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d8d2090f-0187-45ee-b1fb-769698e8b109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Zip the data folder\n",
        "!zip -r data_folder.zip /content/data\n",
        "\n",
        "# Step 2: Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('data_folder.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "DqmupzrR0IQN",
        "outputId": "820c1075-96a8-4fb5-b0e6-8d38490552c9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/data/ (stored 0%)\n",
            "  adding: content/data/parquet/ (stored 0%)\n",
            "  adding: content/data/parquet/orderdetails/ (stored 0%)\n",
            "  adding: content/data/parquet/orderdetails/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/orderdetails/.part-00000-0585ec6f-a703-46f8-b8d5-2fbebbdf3694-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/orderdetails/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/orderdetails/part-00000-0585ec6f-a703-46f8-b8d5-2fbebbdf3694-c000.snappy.parquet (deflated 19%)\n",
            "  adding: content/data/parquet/customers/ (stored 0%)\n",
            "  adding: content/data/parquet/customers/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/customers/part-00000-49f415cd-a570-4422-b382-60b0f014270f-c000.snappy.parquet (deflated 25%)\n",
            "  adding: content/data/parquet/customers/.part-00000-49f415cd-a570-4422-b382-60b0f014270f-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/customers/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/payments/ (stored 0%)\n",
            "  adding: content/data/parquet/payments/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/payments/part-00000-56cfcb60-6d7b-4d7f-a059-2dd321805168-c000.snappy.parquet (deflated 21%)\n",
            "  adding: content/data/parquet/payments/.part-00000-56cfcb60-6d7b-4d7f-a059-2dd321805168-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/payments/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/employees/ (stored 0%)\n",
            "  adding: content/data/parquet/employees/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/employees/.part-00000-ca368ea0-d55e-4630-ad72-de7e381afb81-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/employees/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/employees/part-00000-ca368ea0-d55e-4630-ad72-de7e381afb81-c000.snappy.parquet (deflated 35%)\n",
            "  adding: content/data/parquet/orders/ (stored 0%)\n",
            "  adding: content/data/parquet/orders/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/orders/part-00000-1ca5ce9b-1a5a-4a60-a446-3c78fbb8cbd3-c000.snappy.parquet (deflated 37%)\n",
            "  adding: content/data/parquet/orders/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/orders/.part-00000-1ca5ce9b-1a5a-4a60-a446-3c78fbb8cbd3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/offices/ (stored 0%)\n",
            "  adding: content/data/parquet/offices/.part-00000-ad864c78-fe0d-47c5-928f-6475f0bdf3b1-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/offices/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/offices/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/offices/part-00000-ad864c78-fe0d-47c5-928f-6475f0bdf3b1-c000.snappy.parquet (deflated 39%)\n",
            "  adding: content/data/parquet/productlines/ (stored 0%)\n",
            "  adding: content/data/parquet/productlines/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/productlines/part-00000-5c9640ca-ca2a-4bc2-b1a5-58eae0ed49e8-c000.snappy.parquet (deflated 39%)\n",
            "  adding: content/data/parquet/productlines/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/productlines/.part-00000-5c9640ca-ca2a-4bc2-b1a5-58eae0ed49e8-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/products/ (stored 0%)\n",
            "  adding: content/data/parquet/products/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/parquet/products/.part-00000-764e295d-771d-4880-807f-a1f2cf630da9-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/products/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/products/part-00000-764e295d-771d-4880-807f-a1f2cf630da9-c000.snappy.parquet (deflated 26%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2a9f3dc3-8d04-4a35-b1d3-1b7d54085208\", \"data_folder.zip\", 35550)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Product & Order Analysis (25 Marks)"
      ],
      "metadata": {
        "id": "9cNxtqGJ1mRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SparkAnalysis.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SparkAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Product and Order Analysis\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"data/parquet/orderdetails\");\n",
        "        Dataset<Row> orders = spark.read().parquet(\"data/parquet/orders\");\n",
        "        Dataset<Row> products = spark.read().parquet(\"data/parquet/products\");\n",
        "\n",
        "        // Top 10 products by quantity sold\n",
        "        Dataset<Row> topProducts = orderDetails.groupBy(\"productCode\")\n",
        "            .sum(\"quantityOrdered\")\n",
        "            .orderBy(functions.desc(\"sum(quantityOrdered)\"))\n",
        "            .limit(10);\n",
        "\n",
        "        topProducts.show();\n",
        "\n",
        "        // Join for product-wise revenue\n",
        "        Dataset<Row> revenueData = orderDetails\n",
        "            .join(products, \"productCode\")\n",
        "            .join(orders, \"orderNumber\")\n",
        "            .withColumn(\"revenue\", functions.expr(\"quantityOrdered * priceEach\"));\n",
        "\n",
        "        Dataset<Row> productRevenue = revenueData.groupBy(\"productCode\", \"productName\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalRevenue\"))\n",
        "            .orderBy(functions.desc(\"totalRevenue\"));\n",
        "\n",
        "        productRevenue.show();\n",
        "\n",
        "        // Average order value per customer\n",
        "        Dataset<Row> customerAOV = revenueData.groupBy(\"customerNumber\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalSpent\"),\n",
        "                 functions.countDistinct(\"orderNumber\").alias(\"orderCount\"))\n",
        "            .withColumn(\"averageOrderValue\", functions.expr(\"totalSpent / orderCount\"));\n",
        "\n",
        "        customerAOV.show();\n",
        "\n",
        "        // Save results\n",
        "        topProducts.write().mode(\"overwrite\").parquet(\"data/results/top_products\");\n",
        "        productRevenue.write().mode(\"overwrite\").parquet(\"data/results/product_revenue\");\n",
        "        customerAOV.write().mode(\"overwrite\").parquet(\"data/results/customer_aov\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k04bXr5u1usM",
        "outputId": "1cc52846-5610-4c40-df28-a64d2392238d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing SparkAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SparkAnalysis.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" SparkAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3ZKRaqR2Oji",
        "outputId": "882414d0-6554-4df4-9d36-5abe57601bec"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:34:12 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:34:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:34:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:34:13 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:34:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:34:13 INFO SparkContext: Submitted application: Product and Order Analysis\n",
            "25/08/06 05:34:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:34:13 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:34:13 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:34:13 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:34:13 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:34:13 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:34:13 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:34:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:34:13 INFO Utils: Successfully started service 'sparkDriver' on port 36581.\n",
            "25/08/06 05:34:13 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:34:13 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:34:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:34:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:34:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:34:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-15c5586a-dff1-4cf9-80e4-679a93e8f07e\n",
            "25/08/06 05:34:14 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:34:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:34:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:34:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 05:34:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 05:34:14 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 05:34:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:34:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44073.\n",
            "25/08/06 05:34:14 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:44073\n",
            "25/08/06 05:34:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:34:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 44073, None)\n",
            "25/08/06 05:34:14 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:44073 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 44073, None)\n",
            "25/08/06 05:34:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 44073, None)\n",
            "25/08/06 05:34:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 44073, None)\n",
            "25/08/06 05:34:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:34:15 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:34:16 INFO InMemoryFileIndex: It took 109 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:34:17 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:10\n",
            "25/08/06 05:34:17 INFO DAGScheduler: Got job 0 (parquet at SparkAnalysis.java:10) with 1 output partitions\n",
            "25/08/06 05:34:17 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at SparkAnalysis.java:10)\n",
            "25/08/06 05:34:17 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:17 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at SparkAnalysis.java:10), which has no missing parents\n",
            "25/08/06 05:34:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:34:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:34:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0c0ac0b11caf:44073 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:34:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at SparkAnalysis.java:10) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7565 bytes) \n",
            "25/08/06 05:34:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 05:34:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2056 bytes result sent to driver\n",
            "25/08/06 05:34:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 820 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:19 INFO DAGScheduler: ResultStage 0 (parquet at SparkAnalysis.java:10) finished in 1.113 s\n",
            "25/08/06 05:34:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 05:34:19 INFO DAGScheduler: Job 0 finished: parquet at SparkAnalysis.java:10, took 1.216362 s\n",
            "25/08/06 05:34:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0c0ac0b11caf:44073 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:34:22 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:34:23 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:11\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Got job 1 (parquet at SparkAnalysis.java:11) with 1 output partitions\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at SparkAnalysis.java:11)\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at SparkAnalysis.java:11), which has no missing parents\n",
            "25/08/06 05:34:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:34:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:34:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0c0ac0b11caf:44073 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:34:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at SparkAnalysis.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7559 bytes) \n",
            "25/08/06 05:34:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 05:34:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1917 bytes result sent to driver\n",
            "25/08/06 05:34:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:23 INFO DAGScheduler: ResultStage 1 (parquet at SparkAnalysis.java:11) finished in 0.178 s\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Job 1 finished: parquet at SparkAnalysis.java:11, took 0.191463 s\n",
            "25/08/06 05:34:23 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:34:23 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:12\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Got job 2 (parquet at SparkAnalysis.java:12) with 1 output partitions\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at SparkAnalysis.java:12)\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at SparkAnalysis.java:12), which has no missing parents\n",
            "25/08/06 05:34:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:34:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 05:34:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0c0ac0b11caf:44073 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at SparkAnalysis.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes) \n",
            "25/08/06 05:34:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 05:34:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1863 bytes result sent to driver\n",
            "25/08/06 05:34:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 62 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:23 INFO DAGScheduler: ResultStage 2 (parquet at SparkAnalysis.java:12) finished in 0.127 s\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 05:34:23 INFO DAGScheduler: Job 2 finished: parquet at SparkAnalysis.java:12, took 0.146590 s\n",
            "25/08/06 05:34:24 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:34:24 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:34:25 INFO CodeGenerator: Code generated in 536.213246 ms\n",
            "25/08/06 05:34:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.2 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:34:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:34:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:25 INFO SparkContext: Created broadcast 3 from show at SparkAnalysis.java:20\n",
            "25/08/06 05:34:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Registering RDD 9 (show at SparkAnalysis.java:20) as input to shuffle 0\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Got map stage job 3 (show at SparkAnalysis.java:20) with 1 output partitions\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at SparkAnalysis.java:20)\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at show at SparkAnalysis.java:20), which has no missing parents\n",
            "25/08/06 05:34:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 40.4 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:34:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 1767.0 MiB)\n",
            "25/08/06 05:34:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0c0ac0b11caf:44073 (size: 18.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:26 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at show at SparkAnalysis.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 05:34:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 05:34:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0c0ac0b11caf:44073 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 0c0ac0b11caf:44073 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:26 INFO CodeGenerator: Code generated in 44.720158 ms\n",
            "25/08/06 05:34:26 INFO CodeGenerator: Code generated in 12.060823 ms\n",
            "25/08/06 05:34:26 INFO CodeGenerator: Code generated in 18.269937 ms\n",
            "25/08/06 05:34:26 INFO CodeGenerator: Code generated in 24.425665 ms\n",
            "25/08/06 05:34:26 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-33ccf549-86dc-4aa2-ad95-a57ce597ebf1-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 05:34:26 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 05:34:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3016 bytes result sent to driver\n",
            "25/08/06 05:34:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1117 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:27 INFO DAGScheduler: ShuffleMapStage 3 (show at SparkAnalysis.java:20) finished in 1.166 s\n",
            "25/08/06 05:34:27 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:27 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:27 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:27 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:27 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:27 INFO CodeGenerator: Code generated in 34.048736 ms\n",
            "25/08/06 05:34:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:34:27 INFO CodeGenerator: Code generated in 64.960759 ms\n",
            "25/08/06 05:34:27 INFO SparkContext: Starting job: show at SparkAnalysis.java:20\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Got job 4 (show at SparkAnalysis.java:20) with 1 output partitions\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Final stage: ResultStage 5 (show at SparkAnalysis.java:20)\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at show at SparkAnalysis.java:20), which has no missing parents\n",
            "25/08/06 05:34:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 41.2 KiB, free 1767.3 MiB)\n",
            "25/08/06 05:34:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1767.3 MiB)\n",
            "25/08/06 05:34:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0c0ac0b11caf:44073 (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at show at SparkAnalysis.java:20) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:27 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:27 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:27 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "25/08/06 05:34:27 INFO ShuffleBlockFetcherIterator: Getting 1 (6.4 KiB) non-empty blocks including 1 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms\n",
            "25/08/06 05:34:27 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 5818 bytes result sent to driver\n",
            "25/08/06 05:34:27 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 188 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:27 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:27 INFO DAGScheduler: ResultStage 5 (show at SparkAnalysis.java:20) finished in 0.219 s\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 05:34:27 INFO DAGScheduler: Job 4 finished: show at SparkAnalysis.java:20, took 0.266247 s\n",
            "25/08/06 05:34:27 INFO CodeGenerator: Code generated in 13.041095 ms\n",
            "25/08/06 05:34:27 INFO CodeGenerator: Code generated in 13.400634 ms\n",
            "+-----------+--------------------+\n",
            "|productCode|sum(quantityOrdered)|\n",
            "+-----------+--------------------+\n",
            "|   S18_3232|                1808|\n",
            "|   S18_1342|                1111|\n",
            "|  S700_4002|                1085|\n",
            "|   S18_3856|                1076|\n",
            "|   S50_1341|                1074|\n",
            "|   S18_4600|                1061|\n",
            "|   S10_1678|                1057|\n",
            "|   S12_4473|                1056|\n",
            "|   S18_2319|                1053|\n",
            "|   S24_3856|                1052|\n",
            "+-----------+--------------------+\n",
            "\n",
            "25/08/06 05:34:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 05:34:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 05:34:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 05:34:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 05:34:28 INFO CodeGenerator: Code generated in 42.243444 ms\n",
            "25/08/06 05:34:28 INFO CodeGenerator: Code generated in 42.515258 ms\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1766.8 MiB)\n",
            "25/08/06 05:34:28 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:28 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.8 MiB)\n",
            "25/08/06 05:34:28 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:28 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:28 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.9 KiB, free 1766.8 MiB)\n",
            "25/08/06 05:34:28 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1766.8 MiB)\n",
            "25/08/06 05:34:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:28 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:28 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:28 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 05:34:28 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.6 KiB, free 1766.8 MiB)\n",
            "25/08/06 05:34:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.8 MiB)\n",
            "25/08/06 05:34:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:28 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:28 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 05:34:28 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 05:34:28 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-824f0589-0cc2-41ca-918f-12277efaa7fe-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 05:34:29 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-55b41c63-85fa-4ea3-8597-2abb68221c9c-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 05:34:29 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 05:34:29 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 3545 bytes result sent to driver\n",
            "25/08/06 05:34:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 294 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:29 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.313 s\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:29 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 5505 bytes result sent to driver\n",
            "25/08/06 05:34:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.321483 s\n",
            "25/08/06 05:34:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 262 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:29 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.300 s\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 0c0ac0b11caf:44073 in memory (size: 19.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.337275 s\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 38.94058 ms\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 28.097326 ms\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 MiB, free 1735.0 MiB)\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 0c0ac0b11caf:44073 in memory (size: 18.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1026.6 KiB, free 1734.1 MiB)\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1734.1 MiB)\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1734.1 MiB)\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 0c0ac0b11caf:44073 (size: 4.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 0c0ac0b11caf:44073 (size: 3.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 105.626517 ms\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.5 KiB, free 1733.9 MiB)\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.9 MiB)\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO SparkContext: Created broadcast 12 from show at SparkAnalysis.java:32\n",
            "25/08/06 05:34:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Registering RDD 25 (show at SparkAnalysis.java:32) as input to shuffle 1\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Got map stage job 7 (show at SparkAnalysis.java:32) with 1 output partitions\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (show at SparkAnalysis.java:32)\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[25] at show at SparkAnalysis.java:32), which has no missing parents\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 56.2 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:34:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:34:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 0c0ac0b11caf:44073 (size: 24.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[25] at show at SparkAnalysis.java:32) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:29 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:29 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 05:34:29 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 29.205603 ms\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 17.027936 ms\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 18.429318 ms\n",
            "25/08/06 05:34:29 INFO CodeGenerator: Code generated in 13.274278 ms\n",
            "25/08/06 05:34:29 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-33ccf549-86dc-4aa2-ad95-a57ce597ebf1-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 05:34:29 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 05:34:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 4723 bytes result sent to driver\n",
            "25/08/06 05:34:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 328 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:30 INFO DAGScheduler: ShuffleMapStage 8 (show at SparkAnalysis.java:32) finished in 0.347 s\n",
            "25/08/06 05:34:30 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:30 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:30 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:30 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:30 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:30 INFO CodeGenerator: Code generated in 17.882712 ms\n",
            "25/08/06 05:34:30 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:34:30 INFO CodeGenerator: Code generated in 43.094909 ms\n",
            "25/08/06 05:34:30 INFO SparkContext: Starting job: show at SparkAnalysis.java:32\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Got job 8 (show at SparkAnalysis.java:32) with 1 output partitions\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Final stage: ResultStage 10 (show at SparkAnalysis.java:32)\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at show at SparkAnalysis.java:32), which has no missing parents\n",
            "25/08/06 05:34:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 52.3 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:34:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:34:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 0c0ac0b11caf:44073 (size: 23.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:30 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at show at SparkAnalysis.java:32) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:30 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:30 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:30 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)\n",
            "25/08/06 05:34:30 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 05:34:30 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 8800 bytes result sent to driver\n",
            "25/08/06 05:34:30 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 66 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:30 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:30 INFO DAGScheduler: ResultStage 10 (show at SparkAnalysis.java:32) finished in 0.080 s\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 05:34:30 INFO DAGScheduler: Job 8 finished: show at SparkAnalysis.java:32, took 0.096276 s\n",
            "25/08/06 05:34:30 INFO CodeGenerator: Code generated in 18.758516 ms\n",
            "25/08/06 05:34:30 INFO CodeGenerator: Code generated in 22.054358 ms\n",
            "+-----------+--------------------+------------------+\n",
            "|productCode|         productName|      totalRevenue|\n",
            "+-----------+--------------------+------------------+\n",
            "|   S18_3232|1992 Ferrari 360 ...|         276839.98|\n",
            "|   S12_1108|   2001 Ferrari Enzo|         190755.86|\n",
            "|   S10_1949|1952 Alpine Renau...|190017.95999999996|\n",
            "|   S10_4698|2003 Harley-David...|170685.99999999997|\n",
            "|   S12_1099|   1968 Ford Mustang|161531.47999999992|\n",
            "|   S12_3891|    1969 Ford Falcon|         152543.02|\n",
            "|   S18_1662|1980s Black Hawk ...|144959.90999999997|\n",
            "|   S18_2238|1998 Chrysler Ply...|142530.62999999998|\n",
            "|   S18_1749|1917 Grand Tourin...|140535.60000000003|\n",
            "|   S12_2823|    2002 Suzuki XREO|135767.03000000003|\n",
            "|   S24_3856|1956 Porsche 356A...|         134240.71|\n",
            "|   S12_3148|  1969 Corvair Monza|132363.78999999998|\n",
            "|   S18_2795|1928 Mercedes-Ben...|132275.97999999998|\n",
            "|   S18_4721|1957 Corvette Con...|130749.31000000001|\n",
            "|   S10_4757| 1972 Alfa Romeo GTA|127924.31999999999|\n",
            "|   S10_4962|1962 LanciaA Delt...|123123.00999999998|\n",
            "|   S18_4027|1970 Triumph Spit...|         122254.75|\n",
            "|   S18_3482|1976 Ford Gran To...|          121890.6|\n",
            "|   S18_3685|1948 Porsche Type...|         121653.46|\n",
            "|   S12_1666|      1958 Setra Bus|119085.24999999999|\n",
            "+-----------+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 05:34:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 05:34:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 05:34:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 05:34:30 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 05:34:30 INFO CodeGenerator: Code generated in 40.708318 ms\n",
            "25/08/06 05:34:30 INFO CodeGenerator: Code generated in 36.478718 ms\n",
            "25/08/06 05:34:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.1 KiB, free 1733.5 MiB)\n",
            "25/08/06 05:34:30 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.2 KiB, free 1733.3 MiB)\n",
            "25/08/06 05:34:30 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1733.3 MiB)\n",
            "25/08/06 05:34:30 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:30 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:30 INFO SparkContext: Created broadcast 16 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.0 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:31 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:31 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:31 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:31 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 05:34:31 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.6 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:31 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-55b41c63-85fa-4ea3-8597-2abb68221c9c-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 05:34:31 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:31 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:31 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:31 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 05:34:31 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)\n",
            "25/08/06 05:34:31 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 2711 bytes result sent to driver\n",
            "25/08/06 05:34:31 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-824f0589-0cc2-41ca-918f-12277efaa7fe-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 05:34:31 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 51 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:31 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.083 s\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.091014 s\n",
            "25/08/06 05:34:31 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 32.0 MiB, free 1701.2 MiB)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 1853.0 B, free 1701.2 MiB)\n",
            "25/08/06 05:34:31 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 0c0ac0b11caf:44073 (size: 1853.0 B, free: 1767.4 MiB)\n",
            "25/08/06 05:34:31 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:31 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 3460 bytes result sent to driver\n",
            "25/08/06 05:34:31 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 54 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:31 INFO DAGScheduler: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.087 s\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.094439 s\n",
            "25/08/06 05:34:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 1026.6 KiB, free 1700.2 MiB)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 1700.2 MiB)\n",
            "25/08/06 05:34:31 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 0c0ac0b11caf:44073 (size: 3.2 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:31 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:31 INFO CodeGenerator: Code generated in 65.217129 ms\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.5 KiB, free 1700.0 MiB)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1700.0 MiB)\n",
            "25/08/06 05:34:31 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:31 INFO SparkContext: Created broadcast 21 from show at SparkAnalysis.java:40\n",
            "25/08/06 05:34:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Registering RDD 41 (show at SparkAnalysis.java:40) as input to shuffle 2\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Got map stage job 11 (show at SparkAnalysis.java:40) with 1 output partitions\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (show at SparkAnalysis.java:40)\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[41] at show at SparkAnalysis.java:40), which has no missing parents\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 55.5 KiB, free 1699.9 MiB)\n",
            "25/08/06 05:34:31 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1699.9 MiB)\n",
            "25/08/06 05:34:31 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 0c0ac0b11caf:44073 (size: 23.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:31 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[41] at show at SparkAnalysis.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:31 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 05:34:31 INFO Executor: Running task 0.0 in stage 13.0 (TID 11)\n",
            "25/08/06 05:34:31 INFO CodeGenerator: Code generated in 15.817248 ms\n",
            "25/08/06 05:34:31 INFO CodeGenerator: Code generated in 21.54707 ms\n",
            "25/08/06 05:34:31 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-33ccf549-86dc-4aa2-ad95-a57ce597ebf1-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 05:34:31 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 05:34:31 INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 4723 bytes result sent to driver\n",
            "25/08/06 05:34:31 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 386 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:31 INFO DAGScheduler: ShuffleMapStage 13 (show at SparkAnalysis.java:40) finished in 0.407 s\n",
            "25/08/06 05:34:31 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:31 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:31 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:31 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:31 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 119.316051 ms\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Registering RDD 44 (show at SparkAnalysis.java:40) as input to shuffle 3\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Got map stage job 12 (show at SparkAnalysis.java:40) with 1 output partitions\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (show at SparkAnalysis.java:40)\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[44] at show at SparkAnalysis.java:40), which has no missing parents\n",
            "25/08/06 05:34:32 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 69.6 KiB, free 1699.8 MiB)\n",
            "25/08/06 05:34:32 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1699.8 MiB)\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 0c0ac0b11caf:44073 (size: 27.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:32 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[44] at show at SparkAnalysis.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:32 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:32 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 05:34:32 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)\n",
            "25/08/06 05:34:32 INFO ShuffleBlockFetcherIterator: Getting 1 (13.2 KiB) non-empty blocks including 1 (13.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 26.015409 ms\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 12.796932 ms\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 19.440116 ms\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 17.360174 ms\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:32 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 7841 bytes result sent to driver\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:32 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 333 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:32 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:32 INFO DAGScheduler: ShuffleMapStage 15 (show at SparkAnalysis.java:40) finished in 0.354 s\n",
            "25/08/06 05:34:32 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:32 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:32 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:32 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:32 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 0c0ac0b11caf:44073 in memory (size: 24.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:32 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 65.120876 ms\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:32 INFO SparkContext: Starting job: show at SparkAnalysis.java:40\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Got job 13 (show at SparkAnalysis.java:40) with 1 output partitions\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Final stage: ResultStage 18 (show at SparkAnalysis.java:40)\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[47] at show at SparkAnalysis.java:40), which has no missing parents\n",
            "25/08/06 05:34:32 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 61.8 KiB, free 1700.3 MiB)\n",
            "25/08/06 05:34:32 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 26.5 KiB, free 1700.3 MiB)\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 0c0ac0b11caf:44073 (size: 26.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:32 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[47] at show at SparkAnalysis.java:40) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:32 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:32 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 13) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:32 INFO Executor: Running task 0.0 in stage 18.0 (TID 13)\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 0c0ac0b11caf:44073 in memory (size: 4.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:32 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/08/06 05:34:32 INFO Executor: Finished task 0.0 in stage 18.0 (TID 13). 9139 bytes result sent to driver\n",
            "25/08/06 05:34:32 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 13) in 47 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:32 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:32 INFO DAGScheduler: ResultStage 18 (show at SparkAnalysis.java:40) finished in 0.065 s\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
            "25/08/06 05:34:32 INFO DAGScheduler: Job 13 finished: show at SparkAnalysis.java:40, took 0.073885 s\n",
            "25/08/06 05:34:32 INFO CodeGenerator: Code generated in 17.480668 ms\n",
            "+--------------+-----------------+----------+-----------------+\n",
            "|customerNumber|       totalSpent|orderCount|averageOrderValue|\n",
            "+--------------+-----------------+----------+-----------------+\n",
            "|          null|9604190.609999998|       326|29460.70739263803|\n",
            "+--------------+-----------------+----------+-----------------+\n",
            "\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 0c0ac0b11caf:44073 in memory (size: 23.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 05:34:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 0c0ac0b11caf:44073 in memory (size: 3.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:33 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 200.2 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:33 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:33 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:33 INFO SparkContext: Created broadcast 25 from parquet at SparkAnalysis.java:43\n",
            "25/08/06 05:34:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Registering RDD 51 (parquet at SparkAnalysis.java:43) as input to shuffle 4\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Got map stage job 14 (parquet at SparkAnalysis.java:43) with 1 output partitions\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (parquet at SparkAnalysis.java:43)\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[51] at parquet at SparkAnalysis.java:43), which has no missing parents\n",
            "25/08/06 05:34:33 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 40.5 KiB, free 1733.4 MiB)\n",
            "25/08/06 05:34:33 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 1733.4 MiB)\n",
            "25/08/06 05:34:33 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 0c0ac0b11caf:44073 (size: 18.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:33 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[51] at parquet at SparkAnalysis.java:43) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:33 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:33 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 14) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 05:34:33 INFO Executor: Running task 0.0 in stage 19.0 (TID 14)\n",
            "25/08/06 05:34:33 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 0c0ac0b11caf:44073 in memory (size: 23.9 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:33 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-33ccf549-86dc-4aa2-ad95-a57ce597ebf1-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 05:34:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 14). 2930 bytes result sent to driver\n",
            "25/08/06 05:34:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 14) in 71 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:33 INFO DAGScheduler: ShuffleMapStage 19 (parquet at SparkAnalysis.java:43) finished in 0.081 s\n",
            "25/08/06 05:34:33 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:33 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:33 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:33 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:33 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:33 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:34:33 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:43\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Got job 15 (parquet at SparkAnalysis.java:43) with 1 output partitions\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at SparkAnalysis.java:43)\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[55] at parquet at SparkAnalysis.java:43), which has no missing parents\n",
            "25/08/06 05:34:33 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 242.4 KiB, free 1733.2 MiB)\n",
            "25/08/06 05:34:33 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 90.0 KiB, free 1733.1 MiB)\n",
            "25/08/06 05:34:33 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 0c0ac0b11caf:44073 (size: 90.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:33 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[55] at parquet at SparkAnalysis.java:43) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:33 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:33 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 15) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:33 INFO Executor: Running task 0.0 in stage 21.0 (TID 15)\n",
            "25/08/06 05:34:33 INFO ShuffleBlockFetcherIterator: Getting 1 (6.4 KiB) non-empty blocks including 1 (6.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:33 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:34:33 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:34:33 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:34:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"sum(quantityOrdered)\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional int64 sum(quantityOrdered);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:34:33 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 05:34:33 INFO FileOutputCommitter: Saved output of task 'attempt_202508060534335592463255564093181_0021_m_000000_15' to file:/content/data/results/top_products/_temporary/0/task_202508060534335592463255564093181_0021_m_000000\n",
            "25/08/06 05:34:33 INFO SparkHadoopMapRedUtil: attempt_202508060534335592463255564093181_0021_m_000000_15: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:34:33 INFO Executor: Finished task 0.0 in stage 21.0 (TID 15). 7465 bytes result sent to driver\n",
            "25/08/06 05:34:33 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 15) in 303 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:33 INFO DAGScheduler: ResultStage 21 (parquet at SparkAnalysis.java:43) finished in 0.385 s\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:33 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
            "25/08/06 05:34:33 INFO DAGScheduler: Job 15 finished: parquet at SparkAnalysis.java:43, took 0.397675 s\n",
            "25/08/06 05:34:33 INFO FileFormatWriter: Start to commit write Job 82108492-6016-4a66-8020-635482de9603.\n",
            "25/08/06 05:34:33 INFO FileFormatWriter: Write Job 82108492-6016-4a66-8020-635482de9603 committed. Elapsed time: 26 ms.\n",
            "25/08/06 05:34:33 INFO FileFormatWriter: Finished processing stats for write job 82108492-6016-4a66-8020-635482de9603.\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 05:34:33 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 200.2 KiB, free 1732.9 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 200.1 KiB, free 1732.7 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1732.7 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 29 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1732.6 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:34 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[59] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 12.9 KiB, free 1732.6 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1732.6 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[59] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:34 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 16) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 05:34:34 INFO Executor: Running task 0.0 in stage 22.0 (TID 16)\n",
            "25/08/06 05:34:34 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-824f0589-0cc2-41ca-918f-12277efaa7fe-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 05:34:34 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 05:34:34 INFO Executor: Finished task 0.0 in stage 22.0 (TID 16). 3459 bytes result sent to driver\n",
            "25/08/06 05:34:34 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:34 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 16) in 50 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:34 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Final stage: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 13.6 KiB, free 1732.6 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1732.6 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:34 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 17) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 05:34:34 INFO Executor: Running task 0.0 in stage 23.0 (TID 17)\n",
            "25/08/06 05:34:34 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.071 s\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.074433 s\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 1026.6 KiB, free 1731.6 MiB)\n",
            "25/08/06 05:34:34 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-55b41c63-85fa-4ea3-8597-2abb68221c9c-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1731.6 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 0c0ac0b11caf:44073 (size: 3.4 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 32 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:34 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 05:34:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:34 INFO Executor: Finished task 0.0 in stage 23.0 (TID 17). 5419 bytes result sent to driver\n",
            "25/08/06 05:34:34 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 17) in 47 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:34 INFO DAGScheduler: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.058 s\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.063072 s\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 32.0 MiB, free 1699.6 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1699.6 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 0c0ac0b11caf:44073 (size: 4.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 200.5 KiB, free 1699.4 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1699.4 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 34 from parquet at SparkAnalysis.java:44\n",
            "25/08/06 05:34:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Registering RDD 67 (parquet at SparkAnalysis.java:44) as input to shuffle 5\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Got map stage job 18 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[67] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 56.3 KiB, free 1699.3 MiB)\n",
            "25/08/06 05:34:34 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1699.3 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 0c0ac0b11caf:44073 (size: 24.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[67] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:34 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 05:34:34 INFO Executor: Running task 0.0 in stage 24.0 (TID 18)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-33ccf549-86dc-4aa2-ad95-a57ce597ebf1-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:34 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 0c0ac0b11caf:44073 in memory (size: 90.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 0c0ac0b11caf:44073 in memory (size: 18.2 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 0c0ac0b11caf:44073 in memory (size: 27.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:34 INFO Executor: Finished task 0.0 in stage 24.0 (TID 18). 4723 bytes result sent to driver\n",
            "25/08/06 05:34:34 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 379 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:34 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:34 INFO DAGScheduler: ShuffleMapStage 24 (parquet at SparkAnalysis.java:44) finished in 0.480 s\n",
            "25/08/06 05:34:34 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:34 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:34 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:34 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 0c0ac0b11caf:44073 in memory (size: 1853.0 B, free: 1767.3 MiB)\n",
            "25/08/06 05:34:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:34 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:34 INFO CodeGenerator: Code generated in 39.376977 ms\n",
            "25/08/06 05:34:35 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:35 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:44\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Got job 19 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Final stage: ResultStage 26 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[72] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 05:34:35 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 52.8 KiB, free 1732.7 MiB)\n",
            "25/08/06 05:34:35 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 0c0ac0b11caf:44073 in memory (size: 3.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:35 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:34:35 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 0c0ac0b11caf:44073 (size: 23.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:35 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:35 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 0c0ac0b11caf:44073 in memory (size: 26.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[72] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:35 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:35 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 19) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:35 INFO Executor: Running task 0.0 in stage 26.0 (TID 19)\n",
            "25/08/06 05:34:35 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 05:34:35 INFO CodeGenerator: Code generated in 12.959428 ms\n",
            "25/08/06 05:34:35 INFO Executor: Finished task 0.0 in stage 26.0 (TID 19). 10838 bytes result sent to driver\n",
            "25/08/06 05:34:35 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 19) in 106 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:35 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:35 INFO DAGScheduler: ResultStage 26 (parquet at SparkAnalysis.java:44) finished in 0.196 s\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Job 19 finished: parquet at SparkAnalysis.java:44, took 0.212349 s\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Registering RDD 73 (parquet at SparkAnalysis.java:44) as input to shuffle 6\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Got map stage job 20 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[73] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 05:34:35 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 56.2 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:34:35 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 24.7 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:34:35 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 0c0ac0b11caf:44073 (size: 24.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:35 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[73] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:35 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:35 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 20) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 05:34:35 INFO Executor: Running task 0.0 in stage 28.0 (TID 20)\n",
            "25/08/06 05:34:35 INFO ShuffleBlockFetcherIterator: Getting 1 (11.1 KiB) non-empty blocks including 1 (11.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 05:34:35 INFO Executor: Finished task 0.0 in stage 28.0 (TID 20). 7368 bytes result sent to driver\n",
            "25/08/06 05:34:35 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 20) in 141 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:35 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:35 INFO DAGScheduler: ShuffleMapStage 28 (parquet at SparkAnalysis.java:44) finished in 0.175 s\n",
            "25/08/06 05:34:35 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:35 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:35 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:35 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:35 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:35 INFO CodeGenerator: Code generated in 28.032998 ms\n",
            "25/08/06 05:34:35 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:44\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Got job 21 (parquet at SparkAnalysis.java:44) with 1 output partitions\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Final stage: ResultStage 31 (parquet at SparkAnalysis.java:44)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[76] at parquet at SparkAnalysis.java:44), which has no missing parents\n",
            "25/08/06 05:34:35 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 248.8 KiB, free 1733.4 MiB)\n",
            "25/08/06 05:34:35 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 92.8 KiB, free 1733.3 MiB)\n",
            "25/08/06 05:34:35 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 0c0ac0b11caf:44073 (size: 92.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:35 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[76] at parquet at SparkAnalysis.java:44) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:35 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:35 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 21) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:35 INFO Executor: Running task 0.0 in stage 31.0 (TID 21)\n",
            "25/08/06 05:34:35 INFO ShuffleBlockFetcherIterator: Getting 1 (12.5 KiB) non-empty blocks including 1 (12.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "25/08/06 05:34:35 INFO CodeGenerator: Code generated in 26.037271 ms\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:35 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:34:35 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:34:35 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:34:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:34:36 INFO FileOutputCommitter: Saved output of task 'attempt_202508060534354705889427741865631_0031_m_000000_21' to file:/content/data/results/product_revenue/_temporary/0/task_202508060534354705889427741865631_0031_m_000000\n",
            "25/08/06 05:34:36 INFO SparkHadoopMapRedUtil: attempt_202508060534354705889427741865631_0031_m_000000_21: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 05:34:36 INFO Executor: Finished task 0.0 in stage 31.0 (TID 21). 9564 bytes result sent to driver\n",
            "25/08/06 05:34:36 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 21) in 367 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:36 INFO DAGScheduler: ResultStage 31 (parquet at SparkAnalysis.java:44) finished in 0.433 s\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Job 21 finished: parquet at SparkAnalysis.java:44, took 0.449666 s\n",
            "25/08/06 05:34:36 INFO FileFormatWriter: Start to commit write Job 0c7bbb97-3db5-4060-a625-6fa68b1d70b9.\n",
            "25/08/06 05:34:36 INFO FileFormatWriter: Write Job 0c7bbb97-3db5-4060-a625-6fa68b1d70b9 committed. Elapsed time: 53 ms.\n",
            "25/08/06 05:34:36 INFO FileFormatWriter: Finished processing stats for write job 0c7bbb97-3db5-4060-a625-6fa68b1d70b9.\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#16)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#10)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 200.1 KiB, free 1733.1 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 200.2 KiB, free 1732.9 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1732.9 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1732.9 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:36 INFO SparkContext: Created broadcast 40 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:36 INFO SparkContext: Created broadcast 39 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Final stage: ResultStage 32 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[84] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 13.0 KiB, free 1732.9 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1732.9 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:36 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[84] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:36 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 22) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 05:34:36 INFO DAGScheduler: Got job 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Final stage: ResultStage 33 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:36 INFO Executor: Running task 0.0 in stage 32.0 (TID 22)\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[83] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 13.6 KiB, free 1732.8 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1732.8 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 0c0ac0b11caf:44073 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:36 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-55b41c63-85fa-4ea3-8597-2abb68221c9c-c000.snappy.parquet, range: 0-4216, partition values: [empty row]\n",
            "25/08/06 05:34:36 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[83] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:36 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 23) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7983 bytes) \n",
            "25/08/06 05:34:36 INFO Executor: Running task 0.0 in stage 33.0 (TID 23)\n",
            "25/08/06 05:34:36 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-824f0589-0cc2-41ca-918f-12277efaa7fe-c000.snappy.parquet, range: 0-3888, partition values: [empty row]\n",
            "25/08/06 05:34:36 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 05:34:36 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 05:34:36 INFO Executor: Finished task 0.0 in stage 32.0 (TID 22). 2711 bytes result sent to driver\n",
            "25/08/06 05:34:36 INFO Executor: Finished task 0.0 in stage 33.0 (TID 23). 3460 bytes result sent to driver\n",
            "25/08/06 05:34:36 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 22) in 75 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:36 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 23) in 49 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:36 INFO DAGScheduler: ResultStage 32 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.107 s\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:36 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
            "25/08/06 05:34:36 INFO DAGScheduler: ResultStage 33 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.067 s\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Job 23 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.123600 s\n",
            "25/08/06 05:34:36 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.123563 s\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 32.0 MiB, free 1700.8 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 1026.6 KiB, free 1699.8 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1853.0 B, free 1700.8 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 0c0ac0b11caf:44073 (size: 1853.0 B, free: 1767.2 MiB)\n",
            "25/08/06 05:34:36 INFO SparkContext: Created broadcast 43 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 1699.8 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 0c0ac0b11caf:44073 (size: 3.2 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:36 INFO SparkContext: Created broadcast 44 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode),IsNotNull(orderNumber)\n",
            "25/08/06 05:34:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1),isnotnull(orderNumber#0)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 200.5 KiB, free 1699.6 MiB)\n",
            "25/08/06 05:34:36 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1699.6 MiB)\n",
            "25/08/06 05:34:36 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 0c0ac0b11caf:44073 (size: 34.8 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:37 INFO SparkContext: Created broadcast 45 from parquet at SparkAnalysis.java:45\n",
            "25/08/06 05:34:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Registering RDD 88 (parquet at SparkAnalysis.java:45) as input to shuffle 7\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Got map stage job 24 (parquet at SparkAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (parquet at SparkAnalysis.java:45)\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[88] at parquet at SparkAnalysis.java:45), which has no missing parents\n",
            "25/08/06 05:34:37 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 55.7 KiB, free 1699.5 MiB)\n",
            "25/08/06 05:34:37 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1699.5 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 0c0ac0b11caf:44073 (size: 24.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:37 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[88] at parquet at SparkAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:37 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:37 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 24) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7978 bytes) \n",
            "25/08/06 05:34:37 INFO Executor: Running task 0.0 in stage 34.0 (TID 24)\n",
            "25/08/06 05:34:37 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-33ccf549-86dc-4aa2-ad95-a57ce597ebf1-c000.snappy.parquet, range: 0-24147, partition values: [empty row]\n",
            "25/08/06 05:34:37 INFO FilterCompat: Filtering using predicate: and(noteq(productCode, null), noteq(orderNumber, null))\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 0c0ac0b11caf:44073 in memory (size: 4.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 0c0ac0b11caf:44073 in memory (size: 24.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 0c0ac0b11caf:44073 in memory (size: 23.6 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 0c0ac0b11caf:44073 in memory (size: 92.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:37 INFO Executor: Finished task 0.0 in stage 34.0 (TID 24). 4809 bytes result sent to driver\n",
            "25/08/06 05:34:37 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 24) in 369 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:37 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:37 INFO DAGScheduler: ShuffleMapStage 34 (parquet at SparkAnalysis.java:45) finished in 0.393 s\n",
            "25/08/06 05:34:37 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:37 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 0c0ac0b11caf:44073 in memory (size: 3.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:37 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 0c0ac0b11caf:44073 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 0c0ac0b11caf:44073 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 0c0ac0b11caf:44073 in memory (size: 24.0 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Registering RDD 91 (parquet at SparkAnalysis.java:45) as input to shuffle 8\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Got map stage job 25 (parquet at SparkAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (parquet at SparkAnalysis.java:45)\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[91] at parquet at SparkAnalysis.java:45), which has no missing parents\n",
            "25/08/06 05:34:37 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 69.6 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:34:37 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:34:37 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 0c0ac0b11caf:44073 (size: 27.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:34:37 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[91] at parquet at SparkAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:37 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:37 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 25) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 05:34:37 INFO Executor: Running task 0.0 in stage 36.0 (TID 25)\n",
            "25/08/06 05:34:37 INFO ShuffleBlockFetcherIterator: Getting 1 (13.2 KiB) non-empty blocks including 1 (13.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 05:34:37 INFO Executor: Finished task 0.0 in stage 36.0 (TID 25). 7798 bytes result sent to driver\n",
            "25/08/06 05:34:37 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 25) in 104 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:37 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:37 INFO DAGScheduler: ShuffleMapStage 36 (parquet at SparkAnalysis.java:45) finished in 0.124 s\n",
            "25/08/06 05:34:37 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:34:37 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:34:37 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:34:37 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:34:37 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:34:38 INFO CodeGenerator: Code generated in 32.006318 ms\n",
            "25/08/06 05:34:38 INFO SparkContext: Starting job: parquet at SparkAnalysis.java:45\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Got job 26 (parquet at SparkAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at SparkAnalysis.java:45)\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[94] at parquet at SparkAnalysis.java:45), which has no missing parents\n",
            "25/08/06 05:34:38 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 260.0 KiB, free 1733.5 MiB)\n",
            "25/08/06 05:34:38 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 96.4 KiB, free 1733.4 MiB)\n",
            "25/08/06 05:34:38 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 0c0ac0b11caf:44073 (size: 96.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 05:34:38 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[94] at parquet at SparkAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:34:38 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:34:38 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 26) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 05:34:38 INFO Executor: Running task 0.0 in stage 39.0 (TID 26)\n",
            "25/08/06 05:34:38 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:34:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 05:34:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:34:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:34:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:34:38 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:34:38 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:34:38 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:34:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"integer\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalSpent\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderCount\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"averageOrderValue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int32 customerNumber;\n",
            "  optional double totalSpent;\n",
            "  required int64 orderCount;\n",
            "  optional double averageOrderValue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:34:38 INFO FileOutputCommitter: Saved output of task 'attempt_202508060534385072811590857180932_0039_m_000000_26' to file:/content/data/results/customer_aov/_temporary/0/task_202508060534385072811590857180932_0039_m_000000\n",
            "25/08/06 05:34:38 INFO SparkHadoopMapRedUtil: attempt_202508060534385072811590857180932_0039_m_000000_26: Committed. Elapsed time: 0 ms.\n",
            "25/08/06 05:34:38 INFO Executor: Finished task 0.0 in stage 39.0 (TID 26). 10055 bytes result sent to driver\n",
            "25/08/06 05:34:38 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 26) in 201 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 05:34:38 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:34:38 INFO DAGScheduler: ResultStage 39 (parquet at SparkAnalysis.java:45) finished in 0.273 s\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:34:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
            "25/08/06 05:34:38 INFO DAGScheduler: Job 26 finished: parquet at SparkAnalysis.java:45, took 0.283294 s\n",
            "25/08/06 05:34:38 INFO FileFormatWriter: Start to commit write Job 3d547105-b1b4-47ed-9646-be8efc9db08e.\n",
            "25/08/06 05:34:38 INFO FileFormatWriter: Write Job 3d547105-b1b4-47ed-9646-be8efc9db08e committed. Elapsed time: 23 ms.\n",
            "25/08/06 05:34:38 INFO FileFormatWriter: Finished processing stats for write job 3d547105-b1b4-47ed-9646-be8efc9db08e.\n",
            "25/08/06 05:34:38 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:34:38 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4041\n",
            "25/08/06 05:34:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:34:38 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:34:38 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:34:38 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:34:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:34:38 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:34:38 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:34:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-2646a5a7-eff8-412d-bcf7-eb7571abef96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ps-F7Rav2zRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 3: Regional Sales Insights (20 Marks)"
      ],
      "metadata": {
        "id": "mDOjaRky4QTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Task3.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "\n",
        "public class Task3 {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Task 3 - Regional Sales Insights\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Read input Parquet data\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"data/parquet/employees\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments\");\n",
        "\n",
        "        // Register views\n",
        "        offices.createOrReplaceTempView(\"offices\");\n",
        "        employees.createOrReplaceTempView(\"employees\");\n",
        "        customers.createOrReplaceTempView(\"customers\");\n",
        "        payments.createOrReplaceTempView(\"payments\");\n",
        "\n",
        "        // Task 3.1: Sales per region (country, city)\n",
        "        Dataset<Row> salesPerRegion = spark.sql(\n",
        "            \"SELECT o.country, o.city, \" +\n",
        "            \"COUNT(DISTINCT e.employeeNumber) AS totalEmployees, \" +\n",
        "            \"COUNT(DISTINCT c.customerNumber) AS totalCustomers \" +\n",
        "            \"FROM offices o \" +\n",
        "            \"JOIN employees e ON o.officeCode = e.officeCode \" +\n",
        "            \"JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber \" +\n",
        "            \"GROUP BY o.country, o.city\"\n",
        "        );\n",
        "\n",
        "        salesPerRegion.show();\n",
        "        salesPerRegion.write().mode(\"overwrite\").parquet(\"data/output/task3/sales_per_region\");\n",
        "\n",
        "        // Task 3.2: Total revenue by country\n",
        "        Dataset<Row> revenueByCountry = spark.sql(\n",
        "            \"SELECT c.country, SUM(payments.amount) AS totalRevenue \" +\n",
        "            \"FROM customers c \" +\n",
        "            \"JOIN payments ON c.customerNumber = payments.customerNumber \" +\n",
        "            \"GROUP BY c.country \" +\n",
        "            \"ORDER BY totalRevenue DESC\"\n",
        "        );\n",
        "\n",
        "        revenueByCountry.show();\n",
        "        revenueByCountry.write().mode(\"overwrite\").parquet(\"data/output/task3/revenue_by_country\");\n",
        "\n",
        "        // Task 3.3: Top-performing offices by revenue\n",
        "        Dataset<Row> topOffices = spark.sql(\n",
        "            \"SELECT o.officeCode, o.city, o.country, SUM(payments.amount) AS officeRevenue \" +\n",
        "            \"FROM offices o \" +\n",
        "            \"JOIN employees e ON o.officeCode = e.officeCode \" +\n",
        "            \"JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber \" +\n",
        "            \"JOIN payments ON c.customerNumber = payments.customerNumber \" +\n",
        "            \"GROUP BY o.officeCode, o.city, o.country \" +\n",
        "            \"ORDER BY officeRevenue DESC\"\n",
        "        );\n",
        "\n",
        "        topOffices.show();\n",
        "        topOffices.write().mode(\"overwrite\").parquet(\"data/output/task3/top_offices\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti4U1i4M4Zti",
        "outputId": "b6f27bb9-c18b-4880-9cee-0cccc9971b49"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Task3.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Task3.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" Task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1rtwN6m-gct",
        "outputId": "c5d3e071-e88c-4fae-9e3d-b69ba0c620cb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 06:49:19 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 06:49:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 06:49:20 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:49:20 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 06:49:20 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:49:20 INFO SparkContext: Submitted application: Task 3 - Regional Sales Insights\n",
            "25/08/06 06:49:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 06:49:20 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 06:49:20 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 06:49:20 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 06:49:20 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 06:49:20 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 06:49:20 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 06:49:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 06:49:20 INFO Utils: Successfully started service 'sparkDriver' on port 41821.\n",
            "25/08/06 06:49:20 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 06:49:21 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 06:49:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 06:49:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 06:49:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 06:49:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c74225a4-eacf-4393-9b99-f722e28748f1\n",
            "25/08/06 06:49:21 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 06:49:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 06:49:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 06:49:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 06:49:21 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 06:49:21 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 06:49:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 06:49:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43409.\n",
            "25/08/06 06:49:21 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:43409\n",
            "25/08/06 06:49:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 06:49:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 43409, None)\n",
            "25/08/06 06:49:21 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:43409 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 43409, None)\n",
            "25/08/06 06:49:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 43409, None)\n",
            "25/08/06 06:49:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 43409, None)\n",
            "25/08/06 06:49:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 06:49:22 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 06:49:23 INFO InMemoryFileIndex: It took 102 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:49:24 INFO SparkContext: Starting job: parquet at Task3.java:13\n",
            "25/08/06 06:49:24 INFO DAGScheduler: Got job 0 (parquet at Task3.java:13) with 1 output partitions\n",
            "25/08/06 06:49:24 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Task3.java:13)\n",
            "25/08/06 06:49:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Task3.java:13), which has no missing parents\n",
            "25/08/06 06:49:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:49:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:49:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0c0ac0b11caf:43409 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:49:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Task3.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7560 bytes) \n",
            "25/08/06 06:49:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 06:49:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1851 bytes result sent to driver\n",
            "25/08/06 06:49:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 688 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:25 INFO DAGScheduler: ResultStage 0 (parquet at Task3.java:13) finished in 0.982 s\n",
            "25/08/06 06:49:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 06:49:25 INFO DAGScheduler: Job 0 finished: parquet at Task3.java:13, took 1.119537 s\n",
            "25/08/06 06:49:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0c0ac0b11caf:43409 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:49:28 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:49:28 INFO SparkContext: Starting job: parquet at Task3.java:14\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Got job 1 (parquet at Task3.java:14) with 1 output partitions\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Task3.java:14)\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at Task3.java:14), which has no missing parents\n",
            "25/08/06 06:49:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:49:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:49:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0c0ac0b11caf:43409 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:49:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at Task3.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7562 bytes) \n",
            "25/08/06 06:49:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 06:49:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1915 bytes result sent to driver\n",
            "25/08/06 06:49:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 67 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:28 INFO DAGScheduler: ResultStage 1 (parquet at Task3.java:14) finished in 0.109 s\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Job 1 finished: parquet at Task3.java:14, took 0.126196 s\n",
            "25/08/06 06:49:28 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:49:28 INFO SparkContext: Starting job: parquet at Task3.java:15\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Got job 2 (parquet at Task3.java:15) with 1 output partitions\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Task3.java:15)\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at Task3.java:15), which has no missing parents\n",
            "25/08/06 06:49:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:49:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 06:49:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0c0ac0b11caf:43409 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at Task3.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7562 bytes) \n",
            "25/08/06 06:49:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 06:49:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2391 bytes result sent to driver\n",
            "25/08/06 06:49:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 79 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:29 INFO DAGScheduler: ResultStage 2 (parquet at Task3.java:15) finished in 0.155 s\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Job 2 finished: parquet at Task3.java:15, took 0.169554 s\n",
            "25/08/06 06:49:29 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:49:29 INFO SparkContext: Starting job: parquet at Task3.java:16\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Got job 3 (parquet at Task3.java:16) with 1 output partitions\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at Task3.java:16)\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at Task3.java:16), which has no missing parents\n",
            "25/08/06 06:49:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 102.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 06:49:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.2 MiB)\n",
            "25/08/06 06:49:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0c0ac0b11caf:43409 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at Task3.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes) \n",
            "25/08/06 06:49:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 06:49:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2054 bytes result sent to driver\n",
            "25/08/06 06:49:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 75 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:29 INFO DAGScheduler: ResultStage 3 (parquet at Task3.java:16) finished in 0.129 s\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 06:49:29 INFO DAGScheduler: Job 3 finished: parquet at Task3.java:16, took 0.146338 s\n",
            "25/08/06 06:49:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 0c0ac0b11caf:43409 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0c0ac0b11caf:43409 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:49:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 0c0ac0b11caf:43409 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:49:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 06:49:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 06:49:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 06:49:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 06:49:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 06:49:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23)\n",
            "25/08/06 06:49:33 INFO CodeGenerator: Code generated in 562.841467 ms\n",
            "25/08/06 06:49:33 INFO CodeGenerator: Code generated in 565.584294 ms\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.2 KiB, free 1767.2 MiB)\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:49:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:49:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:33 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:33 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:49:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:33 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:49:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1767.1 MiB)\n",
            "25/08/06 06:49:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:33 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:33 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 06:49:34 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-ffeb0753-aa70-413e-abf2-2d3bdc88faf7-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 06:49:34 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:49:34 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 06:49:34 INFO FilterCompat: Filtering using predicate: noteq(salesRepEmployeeNumber, null)\n",
            "25/08/06 06:49:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1795 bytes result sent to driver\n",
            "25/08/06 06:49:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 400 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:34 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.420 s\n",
            "25/08/06 06:49:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 06:49:34 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.471428 s\n",
            "25/08/06 06:49:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.0 B, free 1767.1 MiB)\n",
            "25/08/06 06:49:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 120.0 B, free 1767.1 MiB)\n",
            "25/08/06 06:49:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 0c0ac0b11caf:43409 (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 06:49:34 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "+-------+----+--------------+--------------+\n",
            "|country|city|totalEmployees|totalCustomers|\n",
            "+-------+----+--------------+--------------+\n",
            "+-------+----+--------------+--------------+\n",
            "\n",
            "25/08/06 06:49:34 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 06:49:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 06:49:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 06:49:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 06:49:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 06:49:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 06:49:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.2 KiB, free 1766.9 MiB)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 200.2 KiB, free 1766.7 MiB)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.7 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1766.7 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:35 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.6 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 06:49:35 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.6 KiB, free 1766.6 MiB)\n",
            "25/08/06 06:49:35 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-ffeb0753-aa70-413e-abf2-2d3bdc88faf7-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1766.6 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:35 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 06:49:35 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 06:49:35 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2243 bytes result sent to driver\n",
            "25/08/06 06:49:35 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:35 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2286 bytes result sent to driver\n",
            "25/08/06 06:49:35 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 113 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:35 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.131 s\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.139509 s\n",
            "25/08/06 06:49:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1593 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:35 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.613 s\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.621207 s\n",
            "25/08/06 06:49:35 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 06:49:35 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:49:35 INFO FilterCompat: Filtering using predicate: noteq(salesRepEmployeeNumber, null)\n",
            "25/08/06 06:49:35 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1795 bytes result sent to driver\n",
            "25/08/06 06:49:35 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 75 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:35 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.173 s\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.181664 s\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 24.0 B, free 1766.6 MiB)\n",
            "25/08/06 06:49:35 INFO CodeGenerator: Code generated in 39.638421 ms\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 120.0 B, free 1766.6 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 0c0ac0b11caf:43409 (size: 120.0 B, free: 1767.4 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 MiB, free 1734.6 MiB)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 864.0 B, free 1734.6 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 0c0ac0b11caf:43409 (size: 864.0 B, free: 1767.4 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 32.0 MiB, free 1702.6 MiB)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 864.0 B, free 1702.6 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 0c0ac0b11caf:43409 (size: 864.0 B, free: 1767.4 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:35 INFO SparkContext: Starting job: parquet at Task3.java:36\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Got job 8 (parquet at Task3.java:36) with 1 output partitions\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at Task3.java:36)\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[27] at parquet at Task3.java:36), which has no missing parents\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 205.1 KiB, free 1702.4 MiB)\n",
            "25/08/06 06:49:35 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1702.3 MiB)\n",
            "25/08/06 06:49:35 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 0c0ac0b11caf:43409 (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:35 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[27] at parquet at Task3.java:36) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:35 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:35 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 06:49:35 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:35 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:49:35 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:49:35 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:49:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalEmployees\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalCustomers\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional binary city (STRING);\n",
            "  required int64 totalEmployees;\n",
            "  required int64 totalCustomers;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:49:36 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 06:49:36 INFO FileOutputCommitter: Saved output of task 'attempt_202508060649354438669946800543113_0008_m_000000_8' to file:/content/data/output/task3/sales_per_region/_temporary/0/task_202508060649354438669946800543113_0008_m_000000\n",
            "25/08/06 06:49:36 INFO SparkHadoopMapRedUtil: attempt_202508060649354438669946800543113_0008_m_000000_8: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:49:36 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2291 bytes result sent to driver\n",
            "25/08/06 06:49:36 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 252 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:36 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:36 INFO DAGScheduler: ResultStage 8 (parquet at Task3.java:36) finished in 0.311 s\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Job 8 finished: parquet at Task3.java:36, took 0.316883 s\n",
            "25/08/06 06:49:36 INFO FileFormatWriter: Start to commit write Job 25906e3a-28ba-4bd3-8910-9b8af4c8f9df.\n",
            "25/08/06 06:49:36 INFO FileFormatWriter: Write Job 25906e3a-28ba-4bd3-8910-9b8af4c8f9df committed. Elapsed time: 25 ms.\n",
            "25/08/06 06:49:36 INFO FileFormatWriter: Finished processing stats for write job 25906e3a-28ba-4bd3-8910-9b8af4c8f9df.\n",
            "25/08/06 06:49:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#12)\n",
            "25/08/06 06:49:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 06:49:36 INFO CodeGenerator: Code generated in 50.693721 ms\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 200.2 KiB, free 1702.2 MiB)\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1702.1 MiB)\n",
            "25/08/06 06:49:36 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:36 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 13.6 KiB, free 1702.1 MiB)\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1702.1 MiB)\n",
            "25/08/06 06:49:36 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:36 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:36 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:36 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:36 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 06:49:36 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:49:36 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:49:36 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 3277 bytes result sent to driver\n",
            "25/08/06 06:49:36 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 50 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:36 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:36 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.063 s\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 06:49:36 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.071496 s\n",
            "25/08/06 06:49:36 INFO CodeGenerator: Code generated in 12.268648 ms\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1027.1 KiB, free 1701.1 MiB)\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1701.1 MiB)\n",
            "25/08/06 06:49:36 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 0c0ac0b11caf:43409 (size: 2.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:36 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 06:49:36 INFO CodeGenerator: Code generated in 135.563781 ms\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 200.2 KiB, free 1700.9 MiB)\n",
            "25/08/06 06:49:36 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1700.9 MiB)\n",
            "25/08/06 06:49:36 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:36 INFO SparkContext: Created broadcast 20 from show at Task3.java:47\n",
            "25/08/06 06:49:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Registering RDD 35 (show at Task3.java:47) as input to shuffle 0\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Got map stage job 10 (show at Task3.java:47) with 1 output partitions\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (show at Task3.java:47)\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[35] at show at Task3.java:47), which has no missing parents\n",
            "25/08/06 06:49:37 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 48.0 KiB, free 1700.8 MiB)\n",
            "25/08/06 06:49:37 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 1700.8 MiB)\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 0c0ac0b11caf:43409 (size: 21.5 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[35] at show at Task3.java:47) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:37 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:37 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7974 bytes) \n",
            "25/08/06 06:49:37 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 06:49:37 INFO CodeGenerator: Code generated in 34.772443 ms\n",
            "25/08/06 06:49:37 INFO CodeGenerator: Code generated in 27.354157 ms\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 0c0ac0b11caf:43409 in memory (size: 864.0 B, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO CodeGenerator: Code generated in 23.053368 ms\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO CodeGenerator: Code generated in 31.903764 ms\n",
            "25/08/06 06:49:37 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 06:49:37 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 3969 bytes result sent to driver\n",
            "25/08/06 06:49:37 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 516 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:37 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:37 INFO DAGScheduler: ShuffleMapStage 10 (show at Task3.java:47) finished in 0.560 s\n",
            "25/08/06 06:49:37 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 0c0ac0b11caf:43409 in memory (size: 120.0 B, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:49:37 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:49:37 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 0c0ac0b11caf:43409 in memory (size: 864.0 B, free: 1767.3 MiB)\n",
            "25/08/06 06:49:37 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 0c0ac0b11caf:43409 in memory (size: 73.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:37 INFO CodeGenerator: Code generated in 24.370982 ms\n",
            "25/08/06 06:49:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:37 INFO CodeGenerator: Code generated in 26.447859 ms\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:37 INFO SparkContext: Starting job: show at Task3.java:47\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Got job 11 (show at Task3.java:47) with 1 output partitions\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Final stage: ResultStage 12 (show at Task3.java:47)\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:37 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at show at Task3.java:47), which has no missing parents\n",
            "25/08/06 06:49:37 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 0c0ac0b11caf:43409 in memory (size: 120.0 B, free: 1767.5 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 46.9 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 0c0ac0b11caf:43409 (size: 21.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at show at Task3.java:47) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 06:49:38 INFO Executor: Running task 0.0 in stage 12.0 (TID 11)\n",
            "25/08/06 06:49:38 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
            "25/08/06 06:49:38 INFO Executor: Finished task 0.0 in stage 12.0 (TID 11). 7347 bytes result sent to driver\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 154 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:38 INFO DAGScheduler: ResultStage 12 (show at Task3.java:47) finished in 0.183 s\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Job 11 finished: show at Task3.java:47, took 0.211603 s\n",
            "25/08/06 06:49:38 INFO CodeGenerator: Code generated in 26.600675 ms\n",
            "25/08/06 06:49:38 INFO CodeGenerator: Code generated in 24.633775 ms\n",
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|        USA|3040029.5199999996|\n",
            "|      Spain| 994438.5300000003|\n",
            "|     France| 965750.5800000001|\n",
            "|  Australia|509385.81999999995|\n",
            "|New Zealand|         392486.59|\n",
            "|         UK|391503.89999999997|\n",
            "|      Italy|325254.55000000005|\n",
            "|    Finland|         295149.35|\n",
            "|  Singapore|261671.59999999998|\n",
            "|     Canada|         205911.86|\n",
            "|    Denmark|          197356.3|\n",
            "|    Germany|         196470.99|\n",
            "|      Japan|         167909.95|\n",
            "|   Norway  |         166621.51|\n",
            "|    Austria|         136119.99|\n",
            "|     Sweden|         120457.09|\n",
            "|Switzerland|         108777.92|\n",
            "|     Norway|         104224.79|\n",
            "|    Belgium|          91471.03|\n",
            "|Philippines|           87468.3|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 06:49:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#12)\n",
            "25/08/06 06:49:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.2 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Got job 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[43] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 13.6 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[43] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:38 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)\n",
            "25/08/06 06:49:38 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:49:38 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:49:38 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 3277 bytes result sent to driver\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 32 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:38 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.045 s\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Job 12 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.054496 s\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 1027.1 KiB, free 1764.7 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1764.7 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 0c0ac0b11caf:43409 (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 200.2 KiB, free 1764.6 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1764.5 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 26 from parquet at Task3.java:48\n",
            "25/08/06 06:49:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Registering RDD 47 (parquet at Task3.java:48) as input to shuffle 1\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Got map stage job 13 (parquet at Task3.java:48) with 1 output partitions\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (parquet at Task3.java:48)\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[47] at parquet at Task3.java:48), which has no missing parents\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.1 KiB, free 1764.5 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 1764.4 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 0c0ac0b11caf:43409 (size: 21.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[47] at parquet at Task3.java:48) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7974 bytes) \n",
            "25/08/06 06:49:38 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)\n",
            "25/08/06 06:49:38 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 06:49:38 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:49:38 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 3883 bytes result sent to driver\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 94 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:38 INFO DAGScheduler: ShuffleMapStage 14 (parquet at Task3.java:48) finished in 0.111 s\n",
            "25/08/06 06:49:38 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:49:38 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:49:38 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:49:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:49:38 INFO CodeGenerator: Code generated in 12.371143 ms\n",
            "25/08/06 06:49:38 INFO SparkContext: Starting job: parquet at Task3.java:48\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Got job 14 (parquet at Task3.java:48) with 1 output partitions\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Final stage: ResultStage 16 (parquet at Task3.java:48)\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[52] at parquet at Task3.java:48), which has no missing parents\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 47.3 KiB, free 1764.4 MiB)\n",
            "25/08/06 06:49:38 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 1764.4 MiB)\n",
            "25/08/06 06:49:38 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 0c0ac0b11caf:43409 (size: 21.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:38 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[52] at parquet at Task3.java:48) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 06:49:38 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)\n",
            "25/08/06 06:49:38 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 06:49:38 INFO CodeGenerator: Code generated in 17.300285 ms\n",
            "25/08/06 06:49:38 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 7114 bytes result sent to driver\n",
            "25/08/06 06:49:38 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 68 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:38 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:38 INFO DAGScheduler: ResultStage 16 (parquet at Task3.java:48) finished in 0.090 s\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Job 14 finished: parquet at Task3.java:48, took 0.106577 s\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Registering RDD 53 (parquet at Task3.java:48) as input to shuffle 2\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Got map stage job 15 (parquet at Task3.java:48) with 1 output partitions\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at Task3.java:48)\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[53] at parquet at Task3.java:48), which has no missing parents\n",
            "25/08/06 06:49:39 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 47.9 KiB, free 1764.3 MiB)\n",
            "25/08/06 06:49:39 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 1764.3 MiB)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 0c0ac0b11caf:43409 (size: 22.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:39 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[53] at parquet at Task3.java:48) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:39 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:39 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
            "25/08/06 06:49:39 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 0c0ac0b11caf:43409 in memory (size: 21.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:39 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 0c0ac0b11caf:43409 in memory (size: 21.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 0c0ac0b11caf:43409 in memory (size: 21.6 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:39 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 6441 bytes result sent to driver\n",
            "25/08/06 06:49:39 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 163 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:39 INFO DAGScheduler: ShuffleMapStage 18 (parquet at Task3.java:48) finished in 0.243 s\n",
            "25/08/06 06:49:39 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:49:39 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:49:39 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:49:39 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:49:39 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:39 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 0c0ac0b11caf:43409 in memory (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 0c0ac0b11caf:43409 in memory (size: 21.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:49:39 INFO CodeGenerator: Code generated in 31.700295 ms\n",
            "25/08/06 06:49:39 INFO SparkContext: Starting job: parquet at Task3.java:48\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Got job 16 (parquet at Task3.java:48) with 1 output partitions\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at Task3.java:48)\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[56] at parquet at Task3.java:48), which has no missing parents\n",
            "25/08/06 06:49:39 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 243.6 KiB, free 1765.8 MiB)\n",
            "25/08/06 06:49:39 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 91.0 KiB, free 1765.7 MiB)\n",
            "25/08/06 06:49:39 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 0c0ac0b11caf:43409 (size: 91.0 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:39 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[56] at parquet at Task3.java:48) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:39 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:39 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 16) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
            "25/08/06 06:49:39 INFO Executor: Running task 0.0 in stage 21.0 (TID 16)\n",
            "25/08/06 06:49:39 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 06:49:39 INFO CodeGenerator: Code generated in 28.780216 ms\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:39 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:49:39 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:49:39 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:49:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:49:39 INFO FileOutputCommitter: Saved output of task 'attempt_202508060649392057200392277658016_0021_m_000000_16' to file:/content/data/output/task3/revenue_by_country/_temporary/0/task_202508060649392057200392277658016_0021_m_000000\n",
            "25/08/06 06:49:39 INFO SparkHadoopMapRedUtil: attempt_202508060649392057200392277658016_0021_m_000000_16: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:49:39 INFO Executor: Finished task 0.0 in stage 21.0 (TID 16). 8724 bytes result sent to driver\n",
            "25/08/06 06:49:39 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 16) in 225 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:39 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:39 INFO DAGScheduler: ResultStage 21 (parquet at Task3.java:48) finished in 0.276 s\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
            "25/08/06 06:49:39 INFO DAGScheduler: Job 16 finished: parquet at Task3.java:48, took 0.285464 s\n",
            "25/08/06 06:49:39 INFO FileFormatWriter: Start to commit write Job de906a8d-2db4-4c79-aca0-9a3435df1db1.\n",
            "25/08/06 06:49:39 INFO FileFormatWriter: Write Job de906a8d-2db4-4c79-aca0-9a3435df1db1 committed. Elapsed time: 27 ms.\n",
            "25/08/06 06:49:39 INFO FileFormatWriter: Finished processing stats for write job de906a8d-2db4-4c79-aca0-9a3435df1db1.\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23),isnotnull(customerNumber#12)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 200.2 KiB, free 1765.5 MiB)\n",
            "25/08/06 06:49:40 INFO CodeGenerator: Code generated in 42.009031 ms\n",
            "25/08/06 06:49:40 INFO CodeGenerator: Code generated in 46.032312 ms\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.3 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 31 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 200.2 KiB, free 1765.1 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 200.2 KiB, free 1765.3 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.1 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 32 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[60] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 13.6 KiB, free 1765.0 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1765.0 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1765.1 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[60] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:40 INFO Executor: Running task 0.0 in stage 22.0 (TID 17)\n",
            "25/08/06 06:49:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-ffeb0753-aa70-413e-abf2-2d3bdc88faf7-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 06:49:40 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 06:49:40 INFO Executor: Finished task 0.0 in stage 22.0 (TID 17). 2200 bytes result sent to driver\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 39 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:40 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.050 s\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.055890 s\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 32.0 MiB, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Got job 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Final stage: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[68] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 13.6 KiB, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 864.0 B, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 0c0ac0b11caf:43409 (size: 864.0 B, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[68] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[66] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:40 INFO Executor: Running task 0.0 in stage 23.0 (TID 18)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 13.6 KiB, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[66] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 06:49:40 INFO Executor: Running task 0.0 in stage 24.0 (TID 19)\n",
            "25/08/06 06:49:40 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 06:49:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 06:49:40 INFO Executor: Finished task 0.0 in stage 23.0 (TID 18). 1752 bytes result sent to driver\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 48 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:40 INFO DAGScheduler: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.081 s\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 18 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.086671 s\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 24.0 B, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 120.0 B, free 1733.0 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 0c0ac0b11caf:43409 (size: 120.0 B, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 38 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:49:40 INFO Executor: Finished task 0.0 in stage 24.0 (TID 19). 4915 bytes result sent to driver\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 64 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:40 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.085 s\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
            "+----------+----+-------+-------------+\n",
            "|officeCode|city|country|officeRevenue|\n",
            "+----------+----+-------+-------------+\n",
            "+----------+----+-------+-------------+\n",
            "\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.122620 s\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 1027.1 KiB, free 1732.0 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1732.0 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 0c0ac0b11caf:43409 (size: 5.0 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 39 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#0)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode),IsNotNull(employeeNumber)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#8),isnotnull(employeeNumber#6)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(salesRepEmployeeNumber),IsNotNull(customerNumber)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(salesRepEmployeeNumber#23),isnotnull(customerNumber#12)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:49:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#38)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 200.2 KiB, free 1731.8 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 200.2 KiB, free 1731.6 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 200.2 KiB, free 1731.4 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1731.4 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 40 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1731.3 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1731.3 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 41 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 0c0ac0b11caf:43409 (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 42 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:49:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 13.6 KiB, free 1731.3 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1731.3 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 20) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:40 INFO Executor: Running task 0.0 in stage 25.0 (TID 20)\n",
            "25/08/06 06:49:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:40 INFO FilterCompat: Filtering using predicate: and(noteq(salesRepEmployeeNumber, null), noteq(customerNumber, null))\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 13.6 KiB, free 1731.3 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1731.3 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[80] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 21) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 06:49:40 INFO Executor: Running task 0.0 in stage 26.0 (TID 21)\n",
            "25/08/06 06:49:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Final stage: ResultStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:40 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[76] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 13.6 KiB, free 1731.2 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1731.2 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 0c0ac0b11caf:43409 (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[76] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:40 INFO Executor: Finished task 0.0 in stage 25.0 (TID 20). 1752 bytes result sent to driver\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 22) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:49:40 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 20) in 84 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:40 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.106 s\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.111704 s\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 24.0 B, free 1731.2 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 120.0 B, free 1731.2 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 0c0ac0b11caf:43409 (size: 120.0 B, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 46 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO Executor: Running task 0.0 in stage 27.0 (TID 22)\n",
            "25/08/06 06:49:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-ffeb0753-aa70-413e-abf2-2d3bdc88faf7-c000.snappy.parquet, range: 0-1518, partition values: [empty row]\n",
            "25/08/06 06:49:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:40 INFO Executor: Finished task 0.0 in stage 26.0 (TID 21). 4915 bytes result sent to driver\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 21) in 97 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:40 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.115 s\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.137016 s\n",
            "25/08/06 06:49:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:40 INFO FilterCompat: Filtering using predicate: and(noteq(officeCode, null), noteq(employeeNumber, null))\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 1027.1 KiB, free 1730.2 MiB)\n",
            "25/08/06 06:49:40 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 1730.2 MiB)\n",
            "25/08/06 06:49:40 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 0c0ac0b11caf:43409 (size: 5.0 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:40 INFO SparkContext: Created broadcast 47 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:40 INFO Executor: Finished task 0.0 in stage 27.0 (TID 22). 2200 bytes result sent to driver\n",
            "25/08/06 06:49:40 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 22) in 96 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:40 INFO DAGScheduler: ResultStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.129 s\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
            "25/08/06 06:49:40 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.186798 s\n",
            "25/08/06 06:49:41 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 32.0 MiB, free 1698.2 MiB)\n",
            "25/08/06 06:49:41 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 864.0 B, free 1698.2 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 0c0ac0b11caf:43409 (size: 864.0 B, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO SparkContext: Created broadcast 48 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 0c0ac0b11caf:43409 in memory (size: 2.5 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 0c0ac0b11caf:43409 in memory (size: 120.0 B, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO SparkContext: Starting job: parquet at Task3.java:62\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Got job 23 (parquet at Task3.java:62) with 1 output partitions\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Final stage: ResultStage 28 (parquet at Task3.java:62)\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[84] at parquet at Task3.java:62), which has no missing parents\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 205.1 KiB, free 1699.3 MiB)\n",
            "25/08/06 06:49:41 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 1699.2 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 0c0ac0b11caf:43409 (size: 73.1 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:49:41 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[84] at parquet at Task3.java:62) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:49:41 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:49:41 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 23) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7384 bytes) \n",
            "25/08/06 06:49:41 INFO Executor: Running task 0.0 in stage 28.0 (TID 23)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:49:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:49:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:49:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:49:41 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:49:41 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:49:41 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:49:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional double officeRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.1 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO FileOutputCommitter: Saved output of task 'attempt_202508060649418405752576015838948_0028_m_000000_23' to file:/content/data/output/task3/top_offices/_temporary/0/task_202508060649418405752576015838948_0028_m_000000\n",
            "25/08/06 06:49:41 INFO SparkHadoopMapRedUtil: attempt_202508060649418405752576015838948_0028_m_000000_23: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 06:49:41 INFO Executor: Finished task 0.0 in stage 28.0 (TID 23). 2248 bytes result sent to driver\n",
            "25/08/06 06:49:41 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 23) in 152 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:49:41 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:49:41 INFO DAGScheduler: ResultStage 28 (parquet at Task3.java:62) finished in 0.227 s\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:49:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO DAGScheduler: Job 23 finished: parquet at Task3.java:62, took 0.231461 s\n",
            "25/08/06 06:49:41 INFO FileFormatWriter: Start to commit write Job ad72bb44-6873-43aa-b5d3-c7de86c24f88.\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 0c0ac0b11caf:43409 in memory (size: 6.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 0c0ac0b11caf:43409 in memory (size: 864.0 B, free: 1767.2 MiB)\n",
            "25/08/06 06:49:41 INFO FileFormatWriter: Write Job ad72bb44-6873-43aa-b5d3-c7de86c24f88 committed. Elapsed time: 36 ms.\n",
            "25/08/06 06:49:41 INFO FileFormatWriter: Finished processing stats for write job ad72bb44-6873-43aa-b5d3-c7de86c24f88.\n",
            "25/08/06 06:49:41 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 06:49:41 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 0c0ac0b11caf:43409 in memory (size: 34.7 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:49:41 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4041\n",
            "25/08/06 06:49:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 06:49:41 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 06:49:41 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 06:49:41 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 06:49:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 06:49:41 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 06:49:41 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 06:49:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-bafb1a39-a27e-425c-a457-c40a0520b819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Performance Optimization"
      ],
      "metadata": {
        "id": "gQvAaVS_IMq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Task4.java\n",
        "import org.apache.spark.api.java.JavaPairRDD;\n",
        "import org.apache.spark.api.java.JavaRDD;\n",
        "import org.apache.spark.api.java.JavaSparkContext;\n",
        "import org.apache.spark.broadcast.Broadcast;\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "import org.apache.spark.storage.StorageLevel;\n",
        "import scala.Tuple2;\n",
        "\n",
        "import java.util.Iterator;\n",
        "\n",
        "public class Task4 {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Task 4: Performance Optimization\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n",
        "\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments\");\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices\");\n",
        "\n",
        "        // Broadcast smaller dataset\n",
        "        Broadcast<Dataset<Row>> broadcastOffices = jsc.broadcast(offices);\n",
        "\n",
        "        // Cache payments since we’ll use it multiple times\n",
        "        payments.persist(StorageLevel.MEMORY_AND_DISK());\n",
        "\n",
        "        // 1. Aggregate revenue per country using mapPartitions\n",
        "        JavaRDD<Row> revenueByCountryRDD = payments\n",
        "                .join(customers, \"customerNumber\")\n",
        "                .select(\"country\", \"amount\")\n",
        "                .javaRDD()\n",
        "                .mapPartitions(iterator -> {\n",
        "                    java.util.Map<String, Double> map = new java.util.HashMap<>();\n",
        "                    while (iterator.hasNext()) {\n",
        "                        Row row = iterator.next();\n",
        "                        String country = row.getString(0);\n",
        "                        double amount = row.getDouble(1);\n",
        "                        map.put(country, map.getOrDefault(country, 0.0) + amount);\n",
        "                    }\n",
        "                    java.util.List<Row> rows = new java.util.ArrayList<>();\n",
        "                    for (java.util.Map.Entry<String, Double> entry : map.entrySet()) {\n",
        "                        rows.add(RowFactory.create(entry.getKey(), entry.getValue()));\n",
        "                    }\n",
        "                    return rows.iterator();\n",
        "                });\n",
        "\n",
        "        // Define schema\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"country\", DataTypes.StringType)\n",
        "                .add(\"totalRevenue\", DataTypes.DoubleType);\n",
        "\n",
        "        Dataset<Row> revenueByCountry = spark.createDataFrame(revenueByCountryRDD, schema);\n",
        "        revenueByCountry.show();\n",
        "\n",
        "        // 2. Aggregate using aggregateByKey\n",
        "        JavaPairRDD<String, Double> countryRevenuePair = payments\n",
        "                .join(customers, \"customerNumber\")\n",
        "                .select(\"country\", \"amount\")\n",
        "                .javaRDD()\n",
        "                .mapToPair(row -> new Tuple2<>(row.getString(0), row.getDouble(1)));\n",
        "\n",
        "        JavaPairRDD<String, Double> aggregatedRevenue = countryRevenuePair.aggregateByKey(\n",
        "                0.0,\n",
        "                Double::sum,\n",
        "                Double::sum\n",
        "        );\n",
        "\n",
        "        Dataset<Row> aggregatedDF = spark.createDataFrame(\n",
        "                aggregatedRevenue.map(tuple -> RowFactory.create(tuple._1, tuple._2)),\n",
        "                schema\n",
        "        );\n",
        "        aggregatedDF.show();\n",
        "\n",
        "        // 3. Lazy evaluation example\n",
        "        Dataset<Row> lazyEval = payments.filter(\"amount > 1000\");\n",
        "        System.out.println(\"Lazy evaluation example defined. Not triggered yet.\");\n",
        "        lazyEval.show(); // Action triggers execution\n",
        "\n",
        "        // Save output\n",
        "        revenueByCountry.write().mode(SaveMode.Overwrite).parquet(\"data/output/task4/revenueByCountry\");\n",
        "        aggregatedDF.write().mode(SaveMode.Overwrite).parquet(\"data/output/task4/aggregatedRevenue\");\n",
        "\n",
        "        // Unpersist after use\n",
        "        payments.unpersist();\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G7om8pB-rxi",
        "outputId": "557648a2-e92c-4f97-ca41-cb59864579c4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Task4.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Task4.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" Task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33FVezpBKbsX",
        "outputId": "2df99534-2322-4aab-902e-48ca07564f2c"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 06:59:19 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 06:59:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 06:59:20 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:59:20 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 06:59:20 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:59:20 INFO SparkContext: Submitted application: Task 4: Performance Optimization\n",
            "25/08/06 06:59:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 06:59:20 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 06:59:20 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 06:59:20 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 06:59:20 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 06:59:20 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 06:59:20 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 06:59:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 06:59:21 INFO Utils: Successfully started service 'sparkDriver' on port 41969.\n",
            "25/08/06 06:59:21 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 06:59:21 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 06:59:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 06:59:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 06:59:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 06:59:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-df41b810-a1c1-41f1-87e1-1b791f1aaf22\n",
            "25/08/06 06:59:22 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 06:59:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 06:59:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 06:59:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 06:59:22 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 06:59:23 INFO Executor: Starting executor ID driver on host 0c0ac0b11caf\n",
            "25/08/06 06:59:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 06:59:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38339.\n",
            "25/08/06 06:59:23 INFO NettyBlockTransferService: Server created on 0c0ac0b11caf:38339\n",
            "25/08/06 06:59:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 06:59:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c0ac0b11caf, 38339, None)\n",
            "25/08/06 06:59:23 INFO BlockManagerMasterEndpoint: Registering block manager 0c0ac0b11caf:38339 with 1767.6 MiB RAM, BlockManagerId(driver, 0c0ac0b11caf, 38339, None)\n",
            "25/08/06 06:59:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c0ac0b11caf, 38339, None)\n",
            "25/08/06 06:59:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c0ac0b11caf, 38339, None)\n",
            "25/08/06 06:59:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 06:59:24 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 06:59:26 INFO InMemoryFileIndex: It took 84 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:59:26 INFO SparkContext: Starting job: parquet at Task4.java:21\n",
            "25/08/06 06:59:26 INFO DAGScheduler: Got job 0 (parquet at Task4.java:21) with 1 output partitions\n",
            "25/08/06 06:59:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at Task4.java:21)\n",
            "25/08/06 06:59:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at Task4.java:21), which has no missing parents\n",
            "25/08/06 06:59:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:59:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:59:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0c0ac0b11caf:38339 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:59:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at Task4.java:21) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7562 bytes) \n",
            "25/08/06 06:59:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 06:59:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2434 bytes result sent to driver\n",
            "25/08/06 06:59:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 859 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:28 INFO DAGScheduler: ResultStage 0 (parquet at Task4.java:21) finished in 1.191 s\n",
            "25/08/06 06:59:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 06:59:28 INFO DAGScheduler: Job 0 finished: parquet at Task4.java:21, took 1.293294 s\n",
            "25/08/06 06:59:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0c0ac0b11caf:38339 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:59:30 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:59:30 INFO SparkContext: Starting job: parquet at Task4.java:22\n",
            "25/08/06 06:59:30 INFO DAGScheduler: Got job 1 (parquet at Task4.java:22) with 1 output partitions\n",
            "25/08/06 06:59:30 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at Task4.java:22)\n",
            "25/08/06 06:59:30 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:30 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at Task4.java:22), which has no missing parents\n",
            "25/08/06 06:59:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.7 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:59:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 06:59:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0c0ac0b11caf:38339 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 06:59:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at Task4.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes) \n",
            "25/08/06 06:59:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 06:59:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2054 bytes result sent to driver\n",
            "25/08/06 06:59:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 54 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:31 INFO DAGScheduler: ResultStage 1 (parquet at Task4.java:22) finished in 0.109 s\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Job 1 finished: parquet at Task4.java:22, took 0.121591 s\n",
            "25/08/06 06:59:31 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:59:31 INFO SparkContext: Starting job: parquet at Task4.java:23\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Got job 2 (parquet at Task4.java:23) with 1 output partitions\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at Task4.java:23)\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at Task4.java:23), which has no missing parents\n",
            "25/08/06 06:59:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.7 KiB, free 1767.4 MiB)\n",
            "25/08/06 06:59:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 06:59:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0c0ac0b11caf:38339 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:59:31 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at Task4.java:23) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7560 bytes) \n",
            "25/08/06 06:59:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 06:59:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1851 bytes result sent to driver\n",
            "25/08/06 06:59:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 46 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:31 INFO DAGScheduler: ResultStage 2 (parquet at Task4.java:23) finished in 0.087 s\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 06:59:31 INFO DAGScheduler: Job 2 finished: parquet at Task4.java:23, took 0.098477 s\n",
            "25/08/06 06:59:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.8 MiB, free 1763.5 MiB)\n",
            "25/08/06 06:59:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1763.5 MiB)\n",
            "25/08/06 06:59:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0c0ac0b11caf:38339 (size: 7.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:59:31 INFO SparkContext: Created broadcast 3 from broadcast at Task4.java:26\n",
            "25/08/06 06:59:32 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:59:32 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:59:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:59:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 06:59:33 INFO CodeGenerator: Code generated in 470.284754 ms\n",
            "25/08/06 06:59:33 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1763.3 MiB)\n",
            "25/08/06 06:59:33 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1763.3 MiB)\n",
            "25/08/06 06:59:33 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0c0ac0b11caf:38339 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:59:33 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:59:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:59:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:59:33 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:59:33 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:59:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:59:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.6 KiB, free 1763.3 MiB)\n",
            "25/08/06 06:59:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1763.2 MiB)\n",
            "25/08/06 06:59:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0c0ac0b11caf:38339 (size: 6.1 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:59:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:59:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 06:59:33 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:59:34 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:59:34 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 06:59:34 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3320 bytes result sent to driver\n",
            "25/08/06 06:59:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 970 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:34 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.988 s\n",
            "25/08/06 06:59:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 06:59:34 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.000833 s\n",
            "25/08/06 06:59:34 INFO CodeGenerator: Code generated in 59.075958 ms\n",
            "25/08/06 06:59:34 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 1027.1 KiB, free 1762.2 MiB)\n",
            "25/08/06 06:59:34 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1762.2 MiB)\n",
            "25/08/06 06:59:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0c0ac0b11caf:38339 (size: 2.5 KiB, free: 1767.5 MiB)\n",
            "25/08/06 06:59:34 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:59:35 INFO CodeGenerator: Code generated in 60.007154 ms\n",
            "25/08/06 06:59:35 INFO CodeGenerator: Code generated in 61.810355 ms\n",
            "25/08/06 06:59:35 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.4 KiB, free 1762.0 MiB)\n",
            "25/08/06 06:59:35 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 1762.0 MiB)\n",
            "25/08/06 06:59:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0c0ac0b11caf:38339 (size: 34.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:35 INFO SparkContext: Created broadcast 7 from javaRDD at Task4.java:35\n",
            "25/08/06 06:59:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:59:35 INFO DefaultCachedBatchSerializer: Predicate isnotnull(customerNumber#26) generates partition filter: ((customerNumber.count#144 - customerNumber.nullCount#143) > 0)\n",
            "25/08/06 06:59:35 INFO CodeGenerator: Code generated in 58.506457 ms\n",
            "25/08/06 06:59:35 INFO SparkContext: Starting job: show at Task4.java:57\n",
            "25/08/06 06:59:35 INFO DAGScheduler: Got job 4 (show at Task4.java:57) with 1 output partitions\n",
            "25/08/06 06:59:35 INFO DAGScheduler: Final stage: ResultStage 4 (show at Task4.java:57)\n",
            "25/08/06 06:59:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:35 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at show at Task4.java:57), which has no missing parents\n",
            "25/08/06 06:59:35 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 0c0ac0b11caf:38339 in memory (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 49.7 KiB, free 1762.0 MiB)\n",
            "25/08/06 06:59:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 1762.0 MiB)\n",
            "25/08/06 06:59:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 0c0ac0b11caf:38339 (size: 21.4 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at show at Task4.java:57) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 06:59:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 06:59:36 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet, range: 0-7229, partition values: [empty row]\n",
            "25/08/06 06:59:36 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 7.3 KiB, free 1762.0 MiB)\n",
            "25/08/06 06:59:36 INFO BlockManagerInfo: Added rdd_14_0 in memory on 0c0ac0b11caf:38339 (size: 7.3 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:36 INFO CodeGenerator: Code generated in 14.437848 ms\n",
            "25/08/06 06:59:36 INFO CodeGenerator: Code generated in 88.56587 ms\n",
            "25/08/06 06:59:36 INFO CodeGenerator: Code generated in 38.372919 ms\n",
            "25/08/06 06:59:36 INFO CodeGenerator: Code generated in 79.067672 ms\n",
            "25/08/06 06:59:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3836 bytes result sent to driver\n",
            "25/08/06 06:59:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 799 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:36 INFO DAGScheduler: ResultStage 4 (show at Task4.java:57) finished in 1.088 s\n",
            "25/08/06 06:59:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 06:59:36 INFO DAGScheduler: Job 4 finished: show at Task4.java:57, took 1.119289 s\n",
            "25/08/06 06:59:36 INFO CodeGenerator: Code generated in 21.976121 ms\n",
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|        USA|3040029.5199999996|\n",
            "|  Singapore|261671.59999999998|\n",
            "|  Hong Kong|          45480.79|\n",
            "|      Japan|         167909.95|\n",
            "|Philippines|           87468.3|\n",
            "|   Norway  |         166621.51|\n",
            "|Switzerland|         108777.92|\n",
            "|      Spain| 994438.5300000003|\n",
            "|New Zealand|         392486.59|\n",
            "|     Canada|         205911.86|\n",
            "|     Sweden|         120457.09|\n",
            "|    Austria|         136119.99|\n",
            "|    Belgium|          91471.03|\n",
            "|     Norway|         104224.79|\n",
            "|         UK|391503.89999999997|\n",
            "|    Ireland|49898.270000000004|\n",
            "|    Finland|         295149.35|\n",
            "|    Denmark|          197356.3|\n",
            "|      Italy|325254.55000000005|\n",
            "|     France| 965750.5800000001|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 06:59:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 06:59:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 06:59:37 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.2 KiB, free 1761.8 MiB)\n",
            "25/08/06 06:59:37 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1761.7 MiB)\n",
            "25/08/06 06:59:37 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 0c0ac0b11caf:38339 (size: 34.7 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:37 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:59:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:59:37 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:59:37 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.6 KiB, free 1761.7 MiB)\n",
            "25/08/06 06:59:37 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 1761.7 MiB)\n",
            "25/08/06 06:59:37 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 0c0ac0b11caf:38339 (size: 6.1 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:37 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:37 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:37 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7986 bytes) \n",
            "25/08/06 06:59:37 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 06:59:37 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet, range: 0-15209, partition values: [empty row]\n",
            "25/08/06 06:59:37 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 06:59:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3277 bytes result sent to driver\n",
            "25/08/06 06:59:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 84 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:37 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.115 s\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.126419 s\n",
            "25/08/06 06:59:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1027.1 KiB, free 1760.7 MiB)\n",
            "25/08/06 06:59:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 1760.7 MiB)\n",
            "25/08/06 06:59:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 0c0ac0b11caf:38339 (size: 2.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:37 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:59:37 INFO DefaultCachedBatchSerializer: Predicate isnotnull(customerNumber#26) generates partition filter: ((customerNumber.count#258 - customerNumber.nullCount#257) > 0)\n",
            "25/08/06 06:59:37 INFO SparkContext: Starting job: show at Task4.java:76\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Registering RDD 37 (mapToPair at Task4.java:64) as input to shuffle 0\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Got job 6 (show at Task4.java:76) with 1 output partitions\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Final stage: ResultStage 7 (show at Task4.java:76)\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
            "25/08/06 06:59:37 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[37] at mapToPair at Task4.java:64), which has no missing parents\n",
            "25/08/06 06:59:38 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 44.8 KiB, free 1760.7 MiB)\n",
            "25/08/06 06:59:38 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1760.6 MiB)\n",
            "25/08/06 06:59:38 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 0c0ac0b11caf:38339 (size: 19.8 KiB, free: 1767.4 MiB)\n",
            "25/08/06 06:59:38 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[37] at mapToPair at Task4.java:64) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:38 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:38 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7974 bytes) \n",
            "25/08/06 06:59:38 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 06:59:38 INFO BlockManager: Found block rdd_14_0 locally\n",
            "25/08/06 06:59:38 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 3077 bytes result sent to driver\n",
            "25/08/06 06:59:38 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 251 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:38 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:38 INFO DAGScheduler: ShuffleMapStage 6 (mapToPair at Task4.java:64) finished in 0.335 s\n",
            "25/08/06 06:59:38 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:59:38 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:59:38 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
            "25/08/06 06:59:38 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:59:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[42] at show at Task4.java:76), which has no missing parents\n",
            "25/08/06 06:59:38 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 23.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 06:59:38 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 06:59:38 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 0c0ac0b11caf:38339 (size: 10.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:38 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[42] at show at Task4.java:76) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "25/08/06 06:59:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 06:59:38 INFO ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:59:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "25/08/06 06:59:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3037 bytes result sent to driver\n",
            "25/08/06 06:59:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 147 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:38 INFO DAGScheduler: ResultStage 7 (show at Task4.java:76) finished in 0.194 s\n",
            "25/08/06 06:59:38 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 06:59:38 INFO DAGScheduler: Job 6 finished: show at Task4.java:76, took 0.699741 s\n",
            "+-----------+------------------+\n",
            "|    country|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|  Australia|509385.81999999995|\n",
            "|         UK|391503.89999999997|\n",
            "|    Belgium|          91471.03|\n",
            "|     Canada|         205911.86|\n",
            "|      Japan|         167909.95|\n",
            "|      Italy|325254.55000000005|\n",
            "|     France| 965750.5800000001|\n",
            "|   Norway  |         166621.51|\n",
            "|    Finland|         295149.35|\n",
            "|Switzerland|         108777.92|\n",
            "|      Spain| 994438.5300000003|\n",
            "|        USA|3040029.5199999996|\n",
            "|  Singapore|261671.59999999998|\n",
            "|    Germany|         196470.99|\n",
            "|  Hong Kong|          45480.79|\n",
            "|    Ireland|49898.270000000004|\n",
            "|New Zealand|         392486.59|\n",
            "|     Sweden|         120457.09|\n",
            "|Philippines|           87468.3|\n",
            "|     Norway|         104224.79|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Lazy evaluation example defined. Not triggered yet.\n",
            "25/08/06 06:59:38 INFO CodeGenerator: Code generated in 40.998972 ms\n",
            "25/08/06 06:59:38 INFO DefaultCachedBatchSerializer: Predicate isnotnull(amount#29) generates partition filter: ((amount.count#385 - amount.nullCount#384) > 0)\n",
            "25/08/06 06:59:39 INFO DefaultCachedBatchSerializer: Predicate (amount#29 > 1000.0) generates partition filter: (1000.0 < amount.upperBound#382)\n",
            "25/08/06 06:59:39 INFO SparkContext: Starting job: show at Task4.java:81\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Got job 7 (show at Task4.java:81) with 1 output partitions\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Final stage: ResultStage 8 (show at Task4.java:81)\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[47] at show at Task4.java:81), which has no missing parents\n",
            "25/08/06 06:59:39 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 06:59:39 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1760.6 MiB)\n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 0c0ac0b11caf:38339 (size: 13.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:39 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[47] at show at Task4.java:81) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:39 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:39 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 06:59:39 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 06:59:39 INFO BlockManager: Found block rdd_14_0 locally\n",
            "25/08/06 06:59:39 INFO CodeGenerator: Code generated in 15.391607 ms\n",
            "25/08/06 06:59:39 INFO CodeGenerator: Code generated in 90.965812 ms\n",
            "25/08/06 06:59:39 INFO Executor: 1 block locks were not released by task 0.0 in stage 8.0 (TID 8)\n",
            "[rdd_14_0]\n",
            "25/08/06 06:59:39 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2843 bytes result sent to driver\n",
            "25/08/06 06:59:39 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 183 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:39 INFO DAGScheduler: ResultStage 8 (show at Task4.java:81) finished in 0.242 s\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Job 7 finished: show at Task4.java:81, took 0.259870 s\n",
            "25/08/06 06:59:39 INFO CodeGenerator: Code generated in 14.596351 ms\n",
            "+--------------+-----------+-----------+---------+\n",
            "|customerNumber|checkNumber|paymentDate|   amount|\n",
            "+--------------+-----------+-----------+---------+\n",
            "|           103|   HQ336336| 2004-10-19|  6066.78|\n",
            "|           103|   JM555205| 2003-06-05| 14571.44|\n",
            "|           103|   OM314933| 2004-12-18|  1676.14|\n",
            "|           112|   BO864823| 2004-12-17| 14191.12|\n",
            "|           112|    HQ55022| 2003-06-06| 32641.98|\n",
            "|           112|   ND748579| 2004-08-20| 33347.88|\n",
            "|           114|    GG31455| 2003-05-20| 45864.03|\n",
            "|           114|   MA765515| 2004-12-15| 82261.22|\n",
            "|           114|   NP603840| 2003-05-31|  7565.08|\n",
            "|           114|    NR27552| 2004-03-10| 44894.74|\n",
            "|           119|   DB933704| 2004-11-14| 19501.82|\n",
            "|           119|   LN373447| 2004-08-08| 47924.19|\n",
            "|           119|    NG94694| 2005-02-22| 49523.67|\n",
            "|           121|   DB889831| 2003-02-16| 50218.95|\n",
            "|           121|   FD317790| 2003-10-28|  1491.38|\n",
            "|           121|   KI831359| 2004-11-04| 17876.32|\n",
            "|           121|   MA302151| 2004-11-28| 34638.14|\n",
            "|           124|   AE215433| 2005-03-05|101244.59|\n",
            "|           124|   BG255406| 2004-08-28| 85410.87|\n",
            "|           124|   CQ287967| 2003-04-11|  11044.3|\n",
            "+--------------+-----------+-----------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 06:59:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:39 INFO CodeGenerator: Code generated in 11.449586 ms\n",
            "25/08/06 06:59:39 INFO SparkContext: Starting job: parquet at Task4.java:84\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Got job 8 (parquet at Task4.java:84) with 1 output partitions\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at Task4.java:84)\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[49] at parquet at Task4.java:84), which has no missing parents\n",
            "25/08/06 06:59:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 249.1 KiB, free 1760.3 MiB)\n",
            "25/08/06 06:59:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 1760.2 MiB)\n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 0c0ac0b11caf:38339 (size: 91.4 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:59:39 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[49] at parquet at Task4.java:84) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:39 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (0c0ac0b11caf, executor driver, partition 0, PROCESS_LOCAL, 7985 bytes) \n",
            "25/08/06 06:59:39 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 06:59:39 INFO BlockManager: Found block rdd_14_0 locally\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:39 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:59:39 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:59:39 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:59:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 0c0ac0b11caf:38339 in memory (size: 21.4 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 0c0ac0b11caf:38339 in memory (size: 10.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:39 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 0c0ac0b11caf:38339 in memory (size: 19.8 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 0c0ac0b11caf:38339 in memory (size: 6.1 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 0c0ac0b11caf:38339 in memory (size: 13.9 KiB, free: 1767.3 MiB)\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: Saved output of task 'attempt_202508060659398006099650914651391_0009_m_000000_9' to file:/content/data/output/task4/revenueByCountry/_temporary/0/task_202508060659398006099650914651391_0009_m_000000\n",
            "25/08/06 06:59:40 INFO SparkHadoopMapRedUtil: attempt_202508060659398006099650914651391_0009_m_000000_9: Committed. Elapsed time: 9 ms.\n",
            "25/08/06 06:59:40 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4205 bytes result sent to driver\n",
            "25/08/06 06:59:40 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 467 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:40 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:40 INFO DAGScheduler: ResultStage 9 (parquet at Task4.java:84) finished in 0.541 s\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Job 8 finished: parquet at Task4.java:84, took 0.550222 s\n",
            "25/08/06 06:59:40 INFO FileFormatWriter: Start to commit write Job 51c10e8b-5f8b-4113-811c-83fc9d8df88a.\n",
            "25/08/06 06:59:40 INFO FileFormatWriter: Write Job 51c10e8b-5f8b-4113-811c-83fc9d8df88a committed. Elapsed time: 23 ms.\n",
            "25/08/06 06:59:40 INFO FileFormatWriter: Finished processing stats for write job 51c10e8b-5f8b-4113-811c-83fc9d8df88a.\n",
            "25/08/06 06:59:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:40 INFO SparkContext: Starting job: parquet at Task4.java:85\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Got job 9 (parquet at Task4.java:85) with 1 output partitions\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at Task4.java:85)\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[51] at parquet at Task4.java:85), which has no missing parents\n",
            "25/08/06 06:59:40 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 223.7 KiB, free 1760.2 MiB)\n",
            "25/08/06 06:59:40 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 81.1 KiB, free 1760.2 MiB)\n",
            "25/08/06 06:59:40 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 0c0ac0b11caf:38339 (size: 81.1 KiB, free: 1767.2 MiB)\n",
            "25/08/06 06:59:40 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[51] at parquet at Task4.java:85) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:59:40 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:59:40 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (0c0ac0b11caf, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "25/08/06 06:59:40 INFO Executor: Running task 0.0 in stage 11.0 (TID 10)\n",
            "25/08/06 06:59:40 INFO ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:59:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:59:40 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:59:40 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:59:40 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 06:59:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:59:40 INFO FileOutputCommitter: Saved output of task 'attempt_202508060659408373834885309969822_0011_m_000000_10' to file:/content/data/output/task4/aggregatedRevenue/_temporary/0/task_202508060659408373834885309969822_0011_m_000000\n",
            "25/08/06 06:59:40 INFO SparkHadoopMapRedUtil: attempt_202508060659408373834885309969822_0011_m_000000_10: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 06:59:40 INFO Executor: Finished task 0.0 in stage 11.0 (TID 10). 3301 bytes result sent to driver\n",
            "25/08/06 06:59:40 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 95 ms on 0c0ac0b11caf (executor driver) (1/1)\n",
            "25/08/06 06:59:40 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:59:40 INFO DAGScheduler: ResultStage 11 (parquet at Task4.java:85) finished in 0.140 s\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:59:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 06:59:40 INFO DAGScheduler: Job 9 finished: parquet at Task4.java:85, took 0.152070 s\n",
            "25/08/06 06:59:40 INFO FileFormatWriter: Start to commit write Job cfa1493b-261b-46ea-b592-b1b7acfd9e92.\n",
            "25/08/06 06:59:40 INFO FileFormatWriter: Write Job cfa1493b-261b-46ea-b592-b1b7acfd9e92 committed. Elapsed time: 22 ms.\n",
            "25/08/06 06:59:40 INFO FileFormatWriter: Finished processing stats for write job cfa1493b-261b-46ea-b592-b1b7acfd9e92.\n",
            "25/08/06 06:59:40 INFO MapPartitionsRDD: Removing RDD 14 from persistence list\n",
            "25/08/06 06:59:40 INFO BlockManager: Removing RDD 14\n",
            "25/08/06 06:59:40 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 06:59:40 INFO SparkUI: Stopped Spark web UI at http://0c0ac0b11caf:4041\n",
            "25/08/06 06:59:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 06:59:40 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 06:59:40 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 06:59:40 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 06:59:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 06:59:40 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 06:59:40 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 06:59:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb504a6b-67b8-4f6b-9d77-cbe81bcd7c06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1RjkwYlKm1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Structure & Submission"
      ],
      "metadata": {
        "id": "q3s0xQkjL4-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Zip the data folder\n",
        "!zip -r data_folder.zip /content/data/\n",
        "\n",
        "# Step 2: Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('data_folder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I99sKkPgL_GG",
        "outputId": "411ff790-dc70-49c3-d53b-ee52acebabb8"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/data/ (stored 0%)\n",
            "updating: content/data/parquet/ (stored 0%)\n",
            "updating: content/data/parquet/orderdetails/ (stored 0%)\n",
            "updating: content/data/parquet/orderdetails/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/orderdetails/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/customers/ (stored 0%)\n",
            "updating: content/data/parquet/customers/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/customers/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/payments/ (stored 0%)\n",
            "updating: content/data/parquet/payments/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/payments/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/employees/ (stored 0%)\n",
            "updating: content/data/parquet/employees/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/employees/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/orders/ (stored 0%)\n",
            "updating: content/data/parquet/orders/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/orders/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/offices/ (stored 0%)\n",
            "updating: content/data/parquet/offices/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/offices/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/productlines/ (stored 0%)\n",
            "updating: content/data/parquet/productlines/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/productlines/_SUCCESS (stored 0%)\n",
            "updating: content/data/parquet/products/ (stored 0%)\n",
            "updating: content/data/parquet/products/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/parquet/products/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/ (stored 0%)\n",
            "updating: content/data/output/product_revenue/ (stored 0%)\n",
            "updating: content/data/output/product_revenue/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/product_revenue/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/product_revenue/.part-00000-f668d9b7-a894-4af0-8c29-1c2453389eba-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/product_revenue/part-00000-f668d9b7-a894-4af0-8c29-1c2453389eba-c000.snappy.parquet (deflated 21%)\n",
            "updating: content/data/output/avg_order_value/ (stored 0%)\n",
            "updating: content/data/output/avg_order_value/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/avg_order_value/.part-00000-36ae37c3-a51e-439d-8c63-4e4eb5372891-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/avg_order_value/part-00000-36ae37c3-a51e-439d-8c63-4e4eb5372891-c000.snappy.parquet (deflated 40%)\n",
            "updating: content/data/output/avg_order_value/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/task3/ (stored 0%)\n",
            "updating: content/data/output/task3/revenue_by_country/ (stored 0%)\n",
            "updating: content/data/output/task3/revenue_by_country/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/task3/revenue_by_country/.part-00000-2fc82656-7ba6-4c74-8b37-c7426fa09e9f-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/task3/revenue_by_country/part-00000-2fc82656-7ba6-4c74-8b37-c7426fa09e9f-c000.snappy.parquet (deflated 27%)\n",
            "updating: content/data/output/task3/revenue_by_country/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/task3/top_offices/ (stored 0%)\n",
            "updating: content/data/output/task3/top_offices/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/task3/top_offices/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/task3/top_offices/part-00000-bf737d7a-2702-4734-b011-3915bcafceb2-c000.snappy.parquet (deflated 48%)\n",
            "updating: content/data/output/task3/top_offices/.part-00000-bf737d7a-2702-4734-b011-3915bcafceb2-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/task3/sales_per_region/ (stored 0%)\n",
            "updating: content/data/output/task3/sales_per_region/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/task3/sales_per_region/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/task3/sales_per_region/part-00000-68a083a2-5014-4173-93ce-16d7977dbbdd-c000.snappy.parquet (deflated 47%)\n",
            "updating: content/data/output/task3/sales_per_region/.part-00000-68a083a2-5014-4173-93ce-16d7977dbbdd-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/top_products/ (stored 0%)\n",
            "updating: content/data/output/top_products/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/top_products/part-00000-b0b94d4e-a603-4146-a307-4fb9237df915-c000.snappy.parquet (deflated 42%)\n",
            "updating: content/data/output/top_products/.part-00000-b0b94d4e-a603-4146-a307-4fb9237df915-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/top_products/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/task4/ (stored 0%)\n",
            "updating: content/data/output/task4/aggregatedRevenue/ (stored 0%)\n",
            "updating: content/data/output/task4/aggregatedRevenue/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/task4/aggregatedRevenue/.part-00000-afb79caf-0322-4da6-934d-0b550d9d09df-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/task4/aggregatedRevenue/_SUCCESS (stored 0%)\n",
            "updating: content/data/output/task4/aggregatedRevenue/part-00000-afb79caf-0322-4da6-934d-0b550d9d09df-c000.snappy.parquet (deflated 26%)\n",
            "updating: content/data/output/task4/revenueByCountry/ (stored 0%)\n",
            "updating: content/data/output/task4/revenueByCountry/._SUCCESS.crc (stored 0%)\n",
            "updating: content/data/output/task4/revenueByCountry/part-00000-dd276cf9-94cb-47e4-b43d-0c1c65daca18-c000.snappy.parquet (deflated 25%)\n",
            "updating: content/data/output/task4/revenueByCountry/.part-00000-dd276cf9-94cb-47e4-b43d-0c1c65daca18-c000.snappy.parquet.crc (stored 0%)\n",
            "updating: content/data/output/task4/revenueByCountry/_SUCCESS (stored 0%)\n",
            "  adding: content/data/results/ (stored 0%)\n",
            "  adding: content/data/results/product_revenue/ (stored 0%)\n",
            "  adding: content/data/results/product_revenue/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/results/product_revenue/part-00000-2a713a70-1567-4f05-b258-863fd07683d5-c000.snappy.parquet (deflated 21%)\n",
            "  adding: content/data/results/product_revenue/.part-00000-2a713a70-1567-4f05-b258-863fd07683d5-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/results/product_revenue/_SUCCESS (stored 0%)\n",
            "  adding: content/data/results/customer_aov/ (stored 0%)\n",
            "  adding: content/data/results/customer_aov/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/results/customer_aov/.part-00000-5fb8686d-3886-4489-8ee3-4074243d85ea-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/results/customer_aov/part-00000-5fb8686d-3886-4489-8ee3-4074243d85ea-c000.snappy.parquet (deflated 49%)\n",
            "  adding: content/data/results/customer_aov/_SUCCESS (stored 0%)\n",
            "  adding: content/data/results/top_products/ (stored 0%)\n",
            "  adding: content/data/results/top_products/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/data/results/top_products/part-00000-4e34e6f9-3741-4792-bf85-ac95efd828a3-c000.snappy.parquet (deflated 36%)\n",
            "  adding: content/data/results/top_products/.part-00000-4e34e6f9-3741-4792-bf85-ac95efd828a3-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/results/top_products/_SUCCESS (stored 0%)\n",
            "  adding: content/data/parquet/orderdetails/part-00000-c2226be0-0e45-4262-8ae9-8401edb38e12-c000.snappy.parquet (deflated 14%)\n",
            "  adding: content/data/parquet/orderdetails/.part-00000-c2226be0-0e45-4262-8ae9-8401edb38e12-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/customers/.part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/customers/part-00000-71da9e17-2e83-4097-848f-b69dce83dd00-c000.snappy.parquet (deflated 29%)\n",
            "  adding: content/data/parquet/payments/part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet (deflated 25%)\n",
            "  adding: content/data/parquet/payments/.part-00000-d90f3d44-a217-4133-a2e0-e37b562bb84b-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/employees/part-00000-ffeb0753-aa70-413e-abf2-2d3bdc88faf7-c000.snappy.parquet (deflated 35%)\n",
            "  adding: content/data/parquet/employees/.part-00000-ffeb0753-aa70-413e-abf2-2d3bdc88faf7-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/orders/.part-00000-29af7391-d564-4ac2-8a32-ee73752d7da9-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/orders/part-00000-29af7391-d564-4ac2-8a32-ee73752d7da9-c000.snappy.parquet (deflated 37%)\n",
            "  adding: content/data/parquet/offices/.part-00000-583f8bd5-6698-47af-b390-361f8cadb1d5-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/offices/part-00000-583f8bd5-6698-47af-b390-361f8cadb1d5-c000.snappy.parquet (deflated 39%)\n",
            "  adding: content/data/parquet/productlines/part-00000-a71c7431-6ded-4029-82c5-d285c4d06f17-c000.snappy.parquet (deflated 39%)\n",
            "  adding: content/data/parquet/productlines/.part-00000-a71c7431-6ded-4029-82c5-d285c4d06f17-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/data/parquet/products/part-00000-2f4c4a5e-ae9a-4568-8434-8863309cb426-c000.snappy.parquet (deflated 26%)\n",
            "  adding: content/data/parquet/products/.part-00000-2f4c4a5e-ae9a-4568-8434-8863309cb426-c000.snappy.parquet.crc (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c6ad44c8-b5b5-4520-98c7-03651a5f5f41\", \"data_folder.zip\", 117421)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4iGvjrBhMI6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}